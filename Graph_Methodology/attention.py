# -*- coding: utf-8 -*-
"""attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10bWzTgWaajE9RAg9tLSULHOXmgnVYAZN
"""

import pandas as pd
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score

class ComplexAttentionModel(nn.Module):
    def __init__(self, input_dim_static, input_dim_temporal, hidden_dim1, hidden_dim2, output_dim):
        super(ComplexAttentionModel, self).__init__()

        # Linear layers for static features
        self.static_linear1 = nn.Linear(input_dim_static, hidden_dim1)
        self.static_bn1 = nn.BatchNorm1d(hidden_dim1)  # Batch Normalization
        self.static_linear2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.static_bn2 = nn.BatchNorm1d(hidden_dim2)

        # Linear layers for temporal features
        self.temporal_linear1 = nn.Linear(input_dim_temporal, hidden_dim1)
        self.temporal_bn1 = nn.BatchNorm1d(hidden_dim1)
        self.temporal_linear2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.temporal_bn2 = nn.BatchNorm1d(hidden_dim2)

        # Attention mechanism
        self.attention = nn.Linear(hidden_dim2, 1)

        # Final linear layer for classification
        self.fc = nn.Linear(hidden_dim2, output_dim)

    def forward(self, static_features, temporal_features):
        # Process static features
        static_output = torch.relu(self.static_bn1(self.static_linear1(static_features)))
        static_output = torch.relu(self.static_bn2(self.static_linear2(static_output)))

        # Process temporal features
        temporal_output = torch.relu(self.temporal_bn1(self.temporal_linear1(temporal_features)))
        temporal_output = torch.relu(self.temporal_bn2(self.temporal_linear2(temporal_output)))

        # Apply attention mechanism
        combined_features = static_output.unsqueeze(1) + temporal_output
        attention_weights = torch.softmax(self.attention(combined_features), dim=1)
        weighted_sum = torch.sum(attention_weights * combined_features, dim=1)

        # Final classification layer
        output = self.fc(weighted_sum)

        return output

# Load your dataset from a CSV file
df = pd.read_csv('features2.csv')

class_mapping = {label: idx for idx, label in enumerate(sorted(pd.unique(df['CPC'])))}
df['CPC_mapped'] = df['CPC'].map(class_mapping)

# Separate static features, temporal features, and mapped target variable
static_features = df[['Age', 'male', 'female', 'other', 'ROSC', 'vfib', 'TTM']].values
temporal_features = df.iloc[:, 8:-2].values  # Assuming 'CPC_mapped' is at the second-to-last column
target_variable = df['CPC_mapped'].values

# Split the data into training and testing sets
X_train_static, X_test_static, X_train_temporal, X_test_temporal, y_train, y_test = train_test_split(
    static_features, temporal_features, target_variable, test_size=0.2, random_state=42
)

# Convert data to PyTorch tensors
X_train_static_tensor = torch.FloatTensor(X_train_static)
X_train_temporal_tensor = torch.FloatTensor(X_train_temporal)
y_train_tensor = torch.LongTensor(y_train)

X_test_static_tensor = torch.FloatTensor(X_test_static)
X_test_temporal_tensor = torch.FloatTensor(X_test_temporal)
y_test_tensor = torch.LongTensor(y_test)

# Instantiate the model
input_dim_static = X_train_static.shape[1]
input_dim_temporal = X_train_temporal.shape[1]
output_dim = len(pd.unique(df['CPC_mapped']))
hidden_dim1 = 64
hidden_dim2 = 32
#model = AttentionModel(input_dim_static, input_dim_temporal, hidden_dim, output_dim)

#base_model = AttentionModel(input_dim_static, input_dim_temporal, hidden_dim, output_dim)
# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Create a PyTorch dataset and loader
train_data = torch.utils.data.TensorDataset(X_train_static_tensor, X_train_temporal_tensor, y_train_tensor)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)

print("Unique Target Values:", torch.unique(target_batch))
print("Missing Values in Target:", target_batch.isnan().sum())

# Instantiate the complex model
complex_model = ComplexAttentionModel(input_dim_static, input_dim_temporal, hidden_dim1, hidden_dim2, output_dim)

# Define a new optimizer and learning rate
new_optimizer = torch.optim.Adam(complex_model.parameters(), lr=0.0001)

# Learning rate schedule
scheduler = torch.optim.lr_scheduler.StepLR(new_optimizer, step_size=30, gamma=0.65)
# Training loop with the complex model and new optimizer
for epoch in range(500):
    complex_model.train()
    for static_batch, temporal_batch, target_batch in train_loader:
        new_optimizer.zero_grad()
        output = complex_model(static_batch, temporal_batch)
        loss = criterion(output, target_batch)
        loss.backward()
        new_optimizer.step()
    # Learning rate schedule step
    scheduler.step()

# ... (rest of the evaluation code with the complex model)

# Evaluation on the test set
complex_model.eval()
with torch.no_grad():
    output_test = complex_model(X_test_static_tensor, X_test_temporal_tensor)
    pred_labels = torch.argmax(output_test, dim=1).numpy()

accuracy = accuracy_score(y_test, pred_labels)
# Compute evaluation metrics
precision = precision_score(y_test, pred_labels, average='weighted')
recall = recall_score(y_test, pred_labels, average='weighted')
f1 = f1_score(y_test, pred_labels, average='weighted')

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

