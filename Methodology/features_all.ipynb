{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9f1ezv3NnKWj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('features_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "U1CZLVOinTem"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Gaussian Process\",\n",
        "         \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n",
        "         \"Naive Bayes\"]\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(5),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
        "    DecisionTreeClassifier(max_depth=3),\n",
        "    RandomForestClassifier(max_depth=3, n_estimators=10),\n",
        "    AdaBoostClassifier(),\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "svH6HWWVo49s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpc_values = df['CPC'].values\n",
        "\n",
        "# Normalize EEG data (110 columns)\n",
        "eeg_data = df.iloc[:, 7:116].values  # these columns are EEG data\n",
        "scaler_eeg = MinMaxScaler()\n",
        "eeg_data_normalized = scaler_eeg.fit_transform(eeg_data)\n",
        "static_features = df.iloc[:, :7].values\n",
        "# Normalize static features\n",
        "scaler_static = MinMaxScaler()\n",
        "static_features_normalized = scaler_static.fit_transform(static_features)\n",
        "# Combine normalized static features, normalized EEG data, and CPC values\n",
        "normalized_data = np.concatenate((static_features_normalized, eeg_data_normalized), axis=1)\n",
        "\n",
        "# Convert CPC values to one-hot encoded labels\n",
        "one_hot_labels = to_categorical(cpc_values - 1, num_classes=5)  # Assuming CPC values start from 1\n",
        "# Train-test split\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    normalized_data, one_hot_labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "gc-sBNFMnq8o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testlab=[]\n",
        "for i in test_labels.tolist():\n",
        "  testlab.append([i.index(1.0)+1])\n",
        "trainlab=[]\n",
        "for i in train_labels.tolist():\n",
        "  trainlab.append([i.index(1.0)+1])"
      ],
      "metadata": {
        "id": "Sxnnd3D-n-uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "# Define model\n",
        "model1 = Sequential()\n",
        "model1.add(LSTM(units=128, input_shape=(116, 1), return_sequences=True))  # Input shape should match the number of EEG columns (117)\n",
        "model1.add(Dropout(0.3))  # Add dropout layer with a dropout rate of 30%\n",
        "model1.add(LSTM(units=64, return_sequences=True))\n",
        "model1.add(LSTM(units=32))\n",
        "model1.add(Dense(units=16, activation='relu'))\n",
        "model1.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "t799agGCoLzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(x=train_data, y=train_labels, epochs=50, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK-DZrgvWImO",
        "outputId": "81e31257-2fa2-4736-bdaf-0ca48903ebb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "13/13 [==============================] - 15s 669ms/step - loss: 1.5031 - accuracy: 0.3953 - val_loss: 1.3021 - val_accuracy: 0.6082\n",
            "Epoch 2/50\n",
            "13/13 [==============================] - 4s 322ms/step - loss: 1.2215 - accuracy: 0.5788 - val_loss: 1.1336 - val_accuracy: 0.6082\n",
            "Epoch 3/50\n",
            "13/13 [==============================] - 4s 319ms/step - loss: 1.1166 - accuracy: 0.5788 - val_loss: 1.0599 - val_accuracy: 0.6082\n",
            "Epoch 4/50\n",
            "13/13 [==============================] - 6s 430ms/step - loss: 1.0670 - accuracy: 0.5788 - val_loss: 1.0278 - val_accuracy: 0.6082\n",
            "Epoch 5/50\n",
            "13/13 [==============================] - 4s 348ms/step - loss: 1.0500 - accuracy: 0.5788 - val_loss: 0.9997 - val_accuracy: 0.6082\n",
            "Epoch 6/50\n",
            "13/13 [==============================] - 5s 411ms/step - loss: 1.0463 - accuracy: 0.5788 - val_loss: 0.9993 - val_accuracy: 0.6082\n",
            "Epoch 7/50\n",
            "13/13 [==============================] - 4s 322ms/step - loss: 1.0419 - accuracy: 0.5788 - val_loss: 0.9831 - val_accuracy: 0.6082\n",
            "Epoch 8/50\n",
            "13/13 [==============================] - 4s 322ms/step - loss: 1.0406 - accuracy: 0.5788 - val_loss: 0.9775 - val_accuracy: 0.6082\n",
            "Epoch 9/50\n",
            "13/13 [==============================] - 6s 459ms/step - loss: 1.0359 - accuracy: 0.5788 - val_loss: 0.9758 - val_accuracy: 0.6082\n",
            "Epoch 10/50\n",
            "13/13 [==============================] - 4s 319ms/step - loss: 1.0335 - accuracy: 0.5788 - val_loss: 0.9745 - val_accuracy: 0.6082\n",
            "Epoch 11/50\n",
            "13/13 [==============================] - 4s 331ms/step - loss: 1.0334 - accuracy: 0.5788 - val_loss: 0.9744 - val_accuracy: 0.6082\n",
            "Epoch 12/50\n",
            "13/13 [==============================] - 6s 459ms/step - loss: 1.0344 - accuracy: 0.5788 - val_loss: 0.9717 - val_accuracy: 0.6082\n",
            "Epoch 13/50\n",
            "13/13 [==============================] - 4s 337ms/step - loss: 1.0380 - accuracy: 0.5788 - val_loss: 0.9714 - val_accuracy: 0.6082\n",
            "Epoch 14/50\n",
            "13/13 [==============================] - 5s 375ms/step - loss: 1.0345 - accuracy: 0.5788 - val_loss: 0.9692 - val_accuracy: 0.6082\n",
            "Epoch 15/50\n",
            "13/13 [==============================] - 5s 359ms/step - loss: 1.0367 - accuracy: 0.5788 - val_loss: 0.9689 - val_accuracy: 0.6082\n",
            "Epoch 16/50\n",
            "13/13 [==============================] - 4s 320ms/step - loss: 1.0340 - accuracy: 0.5788 - val_loss: 0.9708 - val_accuracy: 0.6082\n",
            "Epoch 17/50\n",
            "13/13 [==============================] - 6s 439ms/step - loss: 1.0334 - accuracy: 0.5788 - val_loss: 0.9679 - val_accuracy: 0.6082\n",
            "Epoch 18/50\n",
            "13/13 [==============================] - 4s 329ms/step - loss: 1.0361 - accuracy: 0.5788 - val_loss: 0.9721 - val_accuracy: 0.6082\n",
            "Epoch 19/50\n",
            "13/13 [==============================] - 4s 326ms/step - loss: 1.0327 - accuracy: 0.5788 - val_loss: 0.9718 - val_accuracy: 0.6082\n",
            "Epoch 20/50\n",
            "13/13 [==============================] - 6s 432ms/step - loss: 1.0325 - accuracy: 0.5788 - val_loss: 0.9744 - val_accuracy: 0.6082\n",
            "Epoch 21/50\n",
            "13/13 [==============================] - 5s 355ms/step - loss: 1.0299 - accuracy: 0.5788 - val_loss: 0.9696 - val_accuracy: 0.6082\n",
            "Epoch 22/50\n",
            "13/13 [==============================] - 4s 327ms/step - loss: 1.0300 - accuracy: 0.5788 - val_loss: 0.9707 - val_accuracy: 0.6082\n",
            "Epoch 23/50\n",
            "13/13 [==============================] - 5s 410ms/step - loss: 1.0302 - accuracy: 0.5788 - val_loss: 0.9692 - val_accuracy: 0.6082\n",
            "Epoch 24/50\n",
            "13/13 [==============================] - 4s 337ms/step - loss: 1.0285 - accuracy: 0.5788 - val_loss: 0.9685 - val_accuracy: 0.6082\n",
            "Epoch 25/50\n",
            "13/13 [==============================] - 5s 389ms/step - loss: 1.0253 - accuracy: 0.5788 - val_loss: 0.9640 - val_accuracy: 0.6082\n",
            "Epoch 26/50\n",
            "13/13 [==============================] - 5s 344ms/step - loss: 1.0180 - accuracy: 0.5788 - val_loss: 0.9581 - val_accuracy: 0.6082\n",
            "Epoch 27/50\n",
            "13/13 [==============================] - 4s 325ms/step - loss: 1.0110 - accuracy: 0.5788 - val_loss: 0.9456 - val_accuracy: 0.6082\n",
            "Epoch 28/50\n",
            "13/13 [==============================] - 6s 454ms/step - loss: 0.9946 - accuracy: 0.5788 - val_loss: 0.9119 - val_accuracy: 0.6082\n",
            "Epoch 29/50\n",
            "13/13 [==============================] - 5s 363ms/step - loss: 0.9634 - accuracy: 0.5788 - val_loss: 0.8672 - val_accuracy: 0.6082\n",
            "Epoch 30/50\n",
            "13/13 [==============================] - 4s 330ms/step - loss: 0.9008 - accuracy: 0.5788 - val_loss: 0.7890 - val_accuracy: 0.6082\n",
            "Epoch 31/50\n",
            "13/13 [==============================] - 6s 479ms/step - loss: 0.7996 - accuracy: 0.7519 - val_loss: 0.6321 - val_accuracy: 0.8969\n",
            "Epoch 32/50\n",
            "13/13 [==============================] - 5s 412ms/step - loss: 0.6491 - accuracy: 0.8889 - val_loss: 0.4768 - val_accuracy: 0.8969\n",
            "Epoch 33/50\n",
            "13/13 [==============================] - 6s 452ms/step - loss: 0.5227 - accuracy: 0.8889 - val_loss: 0.3895 - val_accuracy: 0.8969\n",
            "Epoch 34/50\n",
            "13/13 [==============================] - 4s 324ms/step - loss: 0.4431 - accuracy: 0.8889 - val_loss: 0.3505 - val_accuracy: 0.8969\n",
            "Epoch 35/50\n",
            "13/13 [==============================] - 4s 320ms/step - loss: 0.4028 - accuracy: 0.8889 - val_loss: 0.3111 - val_accuracy: 0.8969\n",
            "Epoch 36/50\n",
            "13/13 [==============================] - 5s 429ms/step - loss: 0.3729 - accuracy: 0.8889 - val_loss: 0.3065 - val_accuracy: 0.8969\n",
            "Epoch 37/50\n",
            "13/13 [==============================] - 4s 324ms/step - loss: 0.3541 - accuracy: 0.8889 - val_loss: 0.2873 - val_accuracy: 0.8969\n",
            "Epoch 38/50\n",
            "13/13 [==============================] - 4s 324ms/step - loss: 0.3355 - accuracy: 0.8889 - val_loss: 0.2662 - val_accuracy: 0.8969\n",
            "Epoch 39/50\n",
            "13/13 [==============================] - 6s 452ms/step - loss: 0.3316 - accuracy: 0.8889 - val_loss: 0.2678 - val_accuracy: 0.8969\n",
            "Epoch 40/50\n",
            "13/13 [==============================] - 4s 320ms/step - loss: 0.3346 - accuracy: 0.8915 - val_loss: 0.2414 - val_accuracy: 0.9175\n",
            "Epoch 41/50\n",
            "13/13 [==============================] - 5s 377ms/step - loss: 0.2910 - accuracy: 0.8966 - val_loss: 0.2232 - val_accuracy: 0.9175\n",
            "Epoch 42/50\n",
            "13/13 [==============================] - 5s 395ms/step - loss: 0.2726 - accuracy: 0.9199 - val_loss: 0.2032 - val_accuracy: 0.9175\n",
            "Epoch 43/50\n",
            "13/13 [==============================] - 4s 324ms/step - loss: 0.2558 - accuracy: 0.9199 - val_loss: 0.1743 - val_accuracy: 0.9175\n",
            "Epoch 44/50\n",
            "13/13 [==============================] - 5s 429ms/step - loss: 0.2651 - accuracy: 0.9096 - val_loss: 0.1845 - val_accuracy: 0.8969\n",
            "Epoch 45/50\n",
            "13/13 [==============================] - 4s 348ms/step - loss: 0.2288 - accuracy: 0.9173 - val_loss: 0.1528 - val_accuracy: 0.9175\n",
            "Epoch 46/50\n",
            "13/13 [==============================] - 5s 360ms/step - loss: 0.2277 - accuracy: 0.9432 - val_loss: 0.1596 - val_accuracy: 0.9175\n",
            "Epoch 47/50\n",
            "13/13 [==============================] - 5s 428ms/step - loss: 0.2221 - accuracy: 0.9276 - val_loss: 0.1372 - val_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "13/13 [==============================] - 4s 328ms/step - loss: 0.2007 - accuracy: 0.9561 - val_loss: 0.1283 - val_accuracy: 0.9175\n",
            "Epoch 49/50\n",
            "13/13 [==============================] - 5s 354ms/step - loss: 0.1965 - accuracy: 0.9483 - val_loss: 0.1171 - val_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "13/13 [==============================] - 5s 371ms/step - loss: 0.1884 - accuracy: 0.9483 - val_loss: 0.1488 - val_accuracy: 0.9175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7d3d3ffa3730>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model1.evaluate(x=test_data, y=test_labels)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9kOlXcKXJmR",
        "outputId": "ae5ac405-e819-41db-f592-0d10e1e4f4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 97ms/step - loss: 0.2876 - accuracy: 0.8607\n",
            "Test Loss: 0.28757545351982117, Test Accuracy: 0.8606557250022888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.metrics.mean_squared_error(testlab,ypred1))\n",
        "print(sklearn.metrics.mean_absolute_error(testlab,ypred1))\n",
        "print(sklearn.metrics.f1_score(testlab,ypred1,average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb2ognVH5Cwt",
        "outputId": "f30f5ee4-7b7f-445a-9cb1-2b7b2d8bac62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16393442622950818\n",
            "0.14754098360655737\n",
            "0.8039469637830293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.spatial.distance import euclidean\n",
        "from math import exp\n",
        "\n",
        "\n",
        "\n",
        "gem_matrix_train = np.zeros((train_data.shape[0], train_data.shape[0]))\n",
        "for i in range(train_data.shape[0]):\n",
        "    for j in range(i, train_data.shape[0]):\n",
        "        dtw_distance = np.sqrt(np.sum(np.square(train_data[i] - train_data[j])))\n",
        "        gem_matrix_train[i, j] = exp(-dtw_distance ** 2)\n",
        "        gem_matrix_train[j, i] = gem_matrix_train[i, j]\n",
        "\n",
        "gem_matrix_test = np.zeros((test_data.shape[0], train_data.shape[0]))\n",
        "for i in range(test_data.shape[0]):\n",
        "    for j in range(train_data.shape[0]):\n",
        "        dtw_distance = np.sqrt(np.sum(np.square(test_data[i] - train_data[j])))\n",
        "        gem_matrix_test[i, j] = exp(-dtw_distance ** 2)\n",
        "\n",
        "clf = SVC(kernel='precomputed')\n",
        "clf.fit(gem_matrix_train, trainlab)\n",
        "y_pred = clf.predict(gem_matrix_test)\n",
        "\n",
        "accuracy = accuracy_score(testlab, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Njx5qjUx-j",
        "outputId": "bbb80747-eb29-4f92-ff23-d6f1dd886c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.metrics.mean_squared_error(testlab,y_pred))\n",
        "print(sklearn.metrics.mean_absolute_error(testlab,y_pred))\n",
        "print(sklearn.metrics.f1_score(testlab,y_pred,average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZQ0b6NM43aJ",
        "outputId": "570b2211-c7a0-4e1b-9da0-118f3371bca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2459016393442623\n",
            "0.13114754098360656\n",
            "0.8840477971949977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dwt\n",
        "# stacked generalization with linear meta model on blobs dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from numpy import dstack\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "members=[model1,clf]\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX):\n",
        "  stackX = None\n",
        "  for model in members:\n",
        "    # make prediction\n",
        "    if(model!=clf):\n",
        "      yhat = model.predict(inputX)\n",
        "      print('yhat: ',yhat)\n",
        "      print(type(yhat))\n",
        "    else:\n",
        "      gem_matrix_test = np.zeros((inputX.shape[0], train_data.shape[0]))\n",
        "      for i in range(inputX.shape[0]):\n",
        "          for j in range(train_data.shape[0]):\n",
        "              dtw_distance = np.sqrt(np.sum(np.square(inputX[i] - train_data[j])))\n",
        "              gem_matrix_test[i, j] = exp(-dtw_distance ** 2)\n",
        "      yhat = model.predict(gem_matrix_test)\n",
        "      yhat1=[]\n",
        "      for i in yhat.tolist():\n",
        "        l=[0.0,0.0,0.0,0.0,0.0]\n",
        "        l[i-1]=1.0\n",
        "        yhat1.append(l)\n",
        "      yhat=np.array(yhat1)\n",
        "      print('yhat: ',yhat)\n",
        "      print(type(yhat))\n",
        "    # stack predictions into [rows, members, probabilities]\n",
        "    if stackX is None:\n",
        "      stackX = yhat\n",
        "    else:\n",
        "      stackX = dstack((stackX, yhat))\n",
        "\t# flatten predictions to [rows, members x probabilities]\n",
        "  stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "  return stackX\n",
        "\n",
        "#[1 0 0 0 0]\n",
        "#[0 1 0 0 0]\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model(members, inputX, inputy):\n",
        "  # create dataset using ensemble\n",
        "  stackedX = stacked_dataset(members, inputX)\n",
        "  print('stacked: ',stackedX)\n",
        "  # fit standalone model\n",
        "  #model = LogisticRegression()\n",
        "  #model=clf\n",
        "  model=classifiers[0]\n",
        "  #odel=knn1\n",
        "  #model=rfmodel ----->give 100 percent accuracy\n",
        "  model.fit(stackedX, inputy)\n",
        "  return model\n",
        "\n",
        "# make a prediction with the stacked model\n",
        "def stacked_prediction(members, model, inputX):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# make a prediction\n",
        "\tyhat = model.predict(stackedX)\n",
        "\treturn yhat\n",
        "\n",
        "# fit stacked model using the ensemble\n",
        "model_final = fit_stacked_model(members, test_data, testlab)\n",
        "# evaluate model on test set\n",
        "yhat = stacked_prediction(members, model_final, test_data)\n",
        "acc = accuracy_score(testlab, yhat)\n",
        "print('Stacked Test Accuracy: %.3f' % acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj7fiCEIonxR",
        "outputId": "201b19bf-ea7a-46c7-c760-e834da739300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 26ms/step\n",
            "yhat:  [[0.31242415 0.06281409 0.03737791 0.01490587 0.57247794]\n",
            " [0.31245297 0.06279653 0.0373754  0.01490313 0.572472  ]\n",
            " [0.31733918 0.0631384  0.03764962 0.01494212 0.5669306 ]\n",
            " [0.3124257  0.06281271 0.03737824 0.01490691 0.5724764 ]\n",
            " [0.31247708 0.0627917  0.03737533 0.01490124 0.57245463]\n",
            " [0.3197834  0.06332499 0.03779258 0.01496618 0.56413287]\n",
            " [0.31978348 0.06332459 0.03779301 0.01496745 0.56413144]\n",
            " [0.31238985 0.06282649 0.03737965 0.01491008 0.5724939 ]\n",
            " [0.3222415  0.06350885 0.03793813 0.0149926  0.56131893]\n",
            " [0.31245926 0.06279978 0.03737575 0.01490129 0.5724639 ]\n",
            " [0.31247956 0.06279311 0.03737499 0.01489857 0.5724538 ]\n",
            " [0.31246772 0.06279684 0.03737582 0.01490115 0.5724584 ]\n",
            " [0.3122355  0.06288934 0.03739195 0.01493287 0.57255036]\n",
            " [0.31230026 0.06286221 0.03738612 0.01492255 0.5725288 ]\n",
            " [0.31236535 0.06283744 0.03738181 0.0149138  0.57250166]\n",
            " [0.31477085 0.06300502 0.03751699 0.01493548 0.5697717 ]\n",
            " [0.31970906 0.06335042 0.03779663 0.01497628 0.5641676 ]\n",
            " [0.31967676 0.0633616  0.03779778 0.01497869 0.5641851 ]\n",
            " [0.3124626  0.06279892 0.0373762  0.01490202 0.57246023]\n",
            " [0.3124912  0.06278852 0.03737446 0.0148973  0.5724486 ]\n",
            " [0.3222342  0.06351051 0.03793848 0.01499439 0.5613224 ]\n",
            " [0.31233576 0.06284737 0.0373833  0.01491745 0.5725162 ]\n",
            " [0.3125038  0.06278425 0.03737387 0.01489558 0.5724425 ]\n",
            " [0.31245354 0.06280167 0.03737662 0.01490374 0.57246447]\n",
            " [0.3124851  0.0627901  0.03737523 0.01489946 0.57245016]\n",
            " [0.32226047 0.06350463 0.03793769 0.01499032 0.56130683]\n",
            " [0.31971237 0.06334968 0.03779668 0.01497605 0.5641653 ]\n",
            " [0.31248367 0.06279151 0.03737483 0.01489819 0.57245183]\n",
            " [0.31730208 0.06314854 0.03765078 0.01494575 0.56695294]\n",
            " [0.32230067 0.06348806 0.03793389 0.01498482 0.5612925 ]\n",
            " [0.31246525 0.06279986 0.03737604 0.01490064 0.5724581 ]\n",
            " [0.3124936  0.06278715 0.03737406 0.01489663 0.5724486 ]\n",
            " [0.31244698 0.06280543 0.037377   0.01490344 0.5724672 ]\n",
            " [0.31239876 0.06282105 0.03737939 0.01491026 0.57249045]\n",
            " [0.31250966 0.06277841 0.03737326 0.01489639 0.5724423 ]\n",
            " [0.31230283 0.06286138 0.03738604 0.01492233 0.57252747]\n",
            " [0.31488645 0.06296543 0.03751051 0.01491909 0.56971854]\n",
            " [0.3197757  0.06332604 0.03779307 0.01496812 0.564137  ]\n",
            " [0.31245205 0.06280301 0.03737666 0.01490318 0.57246506]\n",
            " [0.31237486 0.06283126 0.03738103 0.0149138  0.572499  ]\n",
            " [0.31244412 0.06280609 0.03737667 0.0149031  0.5724701 ]\n",
            " [0.3197786  0.06332548 0.03779304 0.0149679  0.56413496]\n",
            " [0.3124037  0.06282206 0.03738005 0.01491032 0.5724839 ]\n",
            " [0.31250343 0.06278423 0.03737392 0.0148957  0.57244265]\n",
            " [0.32231024 0.06348404 0.03793467 0.0149851  0.5612859 ]\n",
            " [0.3124666  0.0627984  0.03737615 0.01490158 0.57245725]\n",
            " [0.322181   0.06353306 0.03794251 0.01500042 0.561343  ]\n",
            " [0.32225487 0.06350727 0.03793802 0.01499087 0.561309  ]\n",
            " [0.31733784 0.06313656 0.03764934 0.01494192 0.5669344 ]\n",
            " [0.32225236 0.06350661 0.03793775 0.01499097 0.5613124 ]\n",
            " [0.3223186  0.06348349 0.03793466 0.01498317 0.56128   ]\n",
            " [0.31244943 0.06280411 0.03737695 0.01490374 0.5724657 ]\n",
            " [0.31241983 0.06281504 0.03737801 0.01490628 0.57248074]\n",
            " [0.31238592 0.06283074 0.03738176 0.01491264 0.572489  ]\n",
            " [0.31243196 0.06280996 0.03737772 0.01490595 0.5724744 ]\n",
            " [0.3221924  0.06353018 0.03794196 0.01499889 0.56133646]\n",
            " [0.3222883  0.0634942  0.03793622 0.01498702 0.5612944 ]\n",
            " [0.31247097 0.06279704 0.03737553 0.01489977 0.57245666]\n",
            " [0.32222104 0.06351825 0.03793951 0.01499477 0.56132656]\n",
            " [0.3124873  0.06278731 0.03737438 0.01489852 0.5724525 ]\n",
            " [0.31231758 0.0628539  0.03738435 0.0149197  0.5725245 ]\n",
            " [0.31241396 0.0628188  0.03737957 0.01490901 0.5724787 ]\n",
            " [0.31233424 0.06284759 0.03738301 0.01491729 0.57251793]\n",
            " [0.3124294  0.06281225 0.03737886 0.01490729 0.5724722 ]\n",
            " [0.31247395 0.06279359 0.03737531 0.01490028 0.5724569 ]\n",
            " [0.32231003 0.06348728 0.03793519 0.01498422 0.56128323]\n",
            " [0.31242254 0.06281342 0.03737817 0.01490707 0.57247883]\n",
            " [0.31977293 0.06332938 0.03779378 0.01496879 0.56413513]\n",
            " [0.32217515 0.06353417 0.03794233 0.01500062 0.5613477 ]\n",
            " [0.32217333 0.06353077 0.03794178 0.01500183 0.5613524 ]\n",
            " [0.31244498 0.06280252 0.03737701 0.01490566 0.57246983]\n",
            " [0.31243458 0.06281018 0.03737768 0.01490484 0.5724727 ]\n",
            " [0.31962797 0.06338353 0.03780276 0.01498686 0.56419873]\n",
            " [0.32230008 0.06349129 0.03793584 0.01498556 0.56128716]\n",
            " [0.32222292 0.06350917 0.03793721 0.01499245 0.5613382 ]\n",
            " [0.3222359  0.06351332 0.03793832 0.01499173 0.5613207 ]\n",
            " [0.3124202  0.06281846 0.03737978 0.01490869 0.5724729 ]\n",
            " [0.32220557 0.06352322 0.03794083 0.01499732 0.561333  ]\n",
            " [0.3122857  0.06286678 0.03738654 0.01492391 0.5725371 ]\n",
            " [0.31236282 0.06283552 0.03738162 0.01491487 0.5725052 ]\n",
            " [0.32225946 0.06350444 0.0379371  0.01498908 0.56130993]\n",
            " [0.31228125 0.06286604 0.03738579 0.01492365 0.5725432 ]\n",
            " [0.3195468  0.06332365 0.03778384 0.01497512 0.5643706 ]\n",
            " [0.31244802 0.06280346 0.03737674 0.01490383 0.572468  ]\n",
            " [0.31244594 0.06280436 0.03737646 0.01490295 0.57247025]\n",
            " [0.31240413 0.06282271 0.0373793  0.01490848 0.5724854 ]\n",
            " [0.31982762 0.06331099 0.03779098 0.01496124 0.56410915]\n",
            " [0.32221457 0.06352147 0.03794067 0.01499644 0.5613268 ]\n",
            " [0.31232986 0.06285188 0.03738488 0.01491948 0.57251394]\n",
            " [0.32227126 0.06350178 0.03793683 0.01498769 0.5613024 ]\n",
            " [0.31247452 0.06279667 0.03737602 0.01490078 0.572452  ]\n",
            " [0.32224795 0.06350562 0.03793681 0.01498946 0.5613201 ]\n",
            " [0.32222033 0.06350646 0.03793707 0.01499407 0.56134206]\n",
            " [0.31980008 0.06331787 0.03779225 0.01496664 0.5641232 ]\n",
            " [0.31242764 0.06281364 0.03737838 0.01490608 0.5724742 ]\n",
            " [0.31250364 0.06278321 0.03737401 0.0148965  0.57244265]\n",
            " [0.32225236 0.06350653 0.03793783 0.01499114 0.5613121 ]\n",
            " [0.31242767 0.0628138  0.03737835 0.01490616 0.572474  ]\n",
            " [0.32224908 0.0635076  0.037938   0.01499152 0.5613138 ]\n",
            " [0.3123324  0.062849   0.03738358 0.01491807 0.572517  ]\n",
            " [0.32223973 0.06351029 0.03793826 0.01499246 0.56131923]\n",
            " [0.31248787 0.0627893  0.0373745  0.01489758 0.5724508 ]\n",
            " [0.31246716 0.06279816 0.03737567 0.01490038 0.57245857]\n",
            " [0.32227975 0.06349697 0.03793598 0.01498653 0.5613007 ]\n",
            " [0.31241623 0.06281798 0.03737891 0.0149074  0.57247955]\n",
            " [0.32228762 0.06349454 0.03793623 0.01498701 0.56129456]\n",
            " [0.31247538 0.06279394 0.0373753  0.01489988 0.57245547]\n",
            " [0.31246206 0.06279956 0.03737588 0.01490095 0.5724616 ]\n",
            " [0.31235248 0.06284067 0.03738183 0.01491482 0.5725102 ]\n",
            " [0.3173178  0.06314296 0.03764997 0.01494383 0.56694543]\n",
            " [0.3222202  0.0635196  0.03793858 0.01499399 0.56132776]\n",
            " [0.31241935 0.06281555 0.03737904 0.01490827 0.57247776]\n",
            " [0.31982034 0.06330176 0.03778888 0.01496192 0.5641271 ]\n",
            " [0.31477442 0.06300066 0.03751618 0.01493499 0.56977373]\n",
            " [0.31247565 0.06279255 0.037375   0.01490026 0.5724565 ]\n",
            " [0.3222745  0.06349862 0.03793658 0.01498828 0.5613019 ]\n",
            " [0.3123635  0.06284386 0.03738404 0.01491603 0.57249254]\n",
            " [0.31247413 0.0627946  0.03737578 0.01490079 0.57245463]\n",
            " [0.3124539  0.06280246 0.03737615 0.0149018  0.57246584]\n",
            " [0.31238866 0.06282838 0.03738044 0.01491088 0.5724917 ]\n",
            " [0.31239387 0.06282628 0.03738103 0.01491317 0.5724856 ]\n",
            " [0.32229364 0.0634923  0.03793588 0.01498621 0.561292  ]]\n",
            "<class 'numpy.ndarray'>\n",
            "yhat:  [[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]]\n",
            "<class 'numpy.ndarray'>\n",
            "stacked:  [[0.31242415 0.         0.06281409 ... 0.         0.57247794 1.        ]\n",
            " [0.31245297 0.         0.06279653 ... 0.         0.57247198 1.        ]\n",
            " [0.31733918 0.         0.0631384  ... 0.         0.56693059 0.        ]\n",
            " ...\n",
            " [0.31238866 0.         0.06282838 ... 0.         0.57249171 1.        ]\n",
            " [0.31239387 0.         0.06282628 ... 0.         0.57248563 1.        ]\n",
            " [0.32229364 1.         0.0634923  ... 0.         0.56129199 0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 19ms/step\n",
            "yhat:  [[0.31242415 0.06281409 0.03737791 0.01490587 0.57247794]\n",
            " [0.31245297 0.06279653 0.0373754  0.01490313 0.572472  ]\n",
            " [0.31733918 0.0631384  0.03764962 0.01494212 0.5669306 ]\n",
            " [0.3124257  0.06281271 0.03737824 0.01490691 0.5724764 ]\n",
            " [0.31247708 0.0627917  0.03737533 0.01490124 0.57245463]\n",
            " [0.3197834  0.06332499 0.03779258 0.01496618 0.56413287]\n",
            " [0.31978348 0.06332459 0.03779301 0.01496745 0.56413144]\n",
            " [0.31238985 0.06282649 0.03737965 0.01491008 0.5724939 ]\n",
            " [0.3222415  0.06350885 0.03793813 0.0149926  0.56131893]\n",
            " [0.31245926 0.06279978 0.03737575 0.01490129 0.5724639 ]\n",
            " [0.31247956 0.06279311 0.03737499 0.01489857 0.5724538 ]\n",
            " [0.31246772 0.06279684 0.03737582 0.01490115 0.5724584 ]\n",
            " [0.3122355  0.06288934 0.03739195 0.01493287 0.57255036]\n",
            " [0.31230026 0.06286221 0.03738612 0.01492255 0.5725288 ]\n",
            " [0.31236535 0.06283744 0.03738181 0.0149138  0.57250166]\n",
            " [0.31477085 0.06300502 0.03751699 0.01493548 0.5697717 ]\n",
            " [0.31970906 0.06335042 0.03779663 0.01497628 0.5641676 ]\n",
            " [0.31967676 0.0633616  0.03779778 0.01497869 0.5641851 ]\n",
            " [0.3124626  0.06279892 0.0373762  0.01490202 0.57246023]\n",
            " [0.3124912  0.06278852 0.03737446 0.0148973  0.5724486 ]\n",
            " [0.3222342  0.06351051 0.03793848 0.01499439 0.5613224 ]\n",
            " [0.31233576 0.06284737 0.0373833  0.01491745 0.5725162 ]\n",
            " [0.3125038  0.06278425 0.03737387 0.01489558 0.5724425 ]\n",
            " [0.31245354 0.06280167 0.03737662 0.01490374 0.57246447]\n",
            " [0.3124851  0.0627901  0.03737523 0.01489946 0.57245016]\n",
            " [0.32226047 0.06350463 0.03793769 0.01499032 0.56130683]\n",
            " [0.31971237 0.06334968 0.03779668 0.01497605 0.5641653 ]\n",
            " [0.31248367 0.06279151 0.03737483 0.01489819 0.57245183]\n",
            " [0.31730208 0.06314854 0.03765078 0.01494575 0.56695294]\n",
            " [0.32230067 0.06348806 0.03793389 0.01498482 0.5612925 ]\n",
            " [0.31246525 0.06279986 0.03737604 0.01490064 0.5724581 ]\n",
            " [0.3124936  0.06278715 0.03737406 0.01489663 0.5724486 ]\n",
            " [0.31244698 0.06280543 0.037377   0.01490344 0.5724672 ]\n",
            " [0.31239876 0.06282105 0.03737939 0.01491026 0.57249045]\n",
            " [0.31250966 0.06277841 0.03737326 0.01489639 0.5724423 ]\n",
            " [0.31230283 0.06286138 0.03738604 0.01492233 0.57252747]\n",
            " [0.31488645 0.06296543 0.03751051 0.01491909 0.56971854]\n",
            " [0.3197757  0.06332604 0.03779307 0.01496812 0.564137  ]\n",
            " [0.31245205 0.06280301 0.03737666 0.01490318 0.57246506]\n",
            " [0.31237486 0.06283126 0.03738103 0.0149138  0.572499  ]\n",
            " [0.31244412 0.06280609 0.03737667 0.0149031  0.5724701 ]\n",
            " [0.3197786  0.06332548 0.03779304 0.0149679  0.56413496]\n",
            " [0.3124037  0.06282206 0.03738005 0.01491032 0.5724839 ]\n",
            " [0.31250343 0.06278423 0.03737392 0.0148957  0.57244265]\n",
            " [0.32231024 0.06348404 0.03793467 0.0149851  0.5612859 ]\n",
            " [0.3124666  0.0627984  0.03737615 0.01490158 0.57245725]\n",
            " [0.322181   0.06353306 0.03794251 0.01500042 0.561343  ]\n",
            " [0.32225487 0.06350727 0.03793802 0.01499087 0.561309  ]\n",
            " [0.31733784 0.06313656 0.03764934 0.01494192 0.5669344 ]\n",
            " [0.32225236 0.06350661 0.03793775 0.01499097 0.5613124 ]\n",
            " [0.3223186  0.06348349 0.03793466 0.01498317 0.56128   ]\n",
            " [0.31244943 0.06280411 0.03737695 0.01490374 0.5724657 ]\n",
            " [0.31241983 0.06281504 0.03737801 0.01490628 0.57248074]\n",
            " [0.31238592 0.06283074 0.03738176 0.01491264 0.572489  ]\n",
            " [0.31243196 0.06280996 0.03737772 0.01490595 0.5724744 ]\n",
            " [0.3221924  0.06353018 0.03794196 0.01499889 0.56133646]\n",
            " [0.3222883  0.0634942  0.03793622 0.01498702 0.5612944 ]\n",
            " [0.31247097 0.06279704 0.03737553 0.01489977 0.57245666]\n",
            " [0.32222104 0.06351825 0.03793951 0.01499477 0.56132656]\n",
            " [0.3124873  0.06278731 0.03737438 0.01489852 0.5724525 ]\n",
            " [0.31231758 0.0628539  0.03738435 0.0149197  0.5725245 ]\n",
            " [0.31241396 0.0628188  0.03737957 0.01490901 0.5724787 ]\n",
            " [0.31233424 0.06284759 0.03738301 0.01491729 0.57251793]\n",
            " [0.3124294  0.06281225 0.03737886 0.01490729 0.5724722 ]\n",
            " [0.31247395 0.06279359 0.03737531 0.01490028 0.5724569 ]\n",
            " [0.32231003 0.06348728 0.03793519 0.01498422 0.56128323]\n",
            " [0.31242254 0.06281342 0.03737817 0.01490707 0.57247883]\n",
            " [0.31977293 0.06332938 0.03779378 0.01496879 0.56413513]\n",
            " [0.32217515 0.06353417 0.03794233 0.01500062 0.5613477 ]\n",
            " [0.32217333 0.06353077 0.03794178 0.01500183 0.5613524 ]\n",
            " [0.31244498 0.06280252 0.03737701 0.01490566 0.57246983]\n",
            " [0.31243458 0.06281018 0.03737768 0.01490484 0.5724727 ]\n",
            " [0.31962797 0.06338353 0.03780276 0.01498686 0.56419873]\n",
            " [0.32230008 0.06349129 0.03793584 0.01498556 0.56128716]\n",
            " [0.32222292 0.06350917 0.03793721 0.01499245 0.5613382 ]\n",
            " [0.3222359  0.06351332 0.03793832 0.01499173 0.5613207 ]\n",
            " [0.3124202  0.06281846 0.03737978 0.01490869 0.5724729 ]\n",
            " [0.32220557 0.06352322 0.03794083 0.01499732 0.561333  ]\n",
            " [0.3122857  0.06286678 0.03738654 0.01492391 0.5725371 ]\n",
            " [0.31236282 0.06283552 0.03738162 0.01491487 0.5725052 ]\n",
            " [0.32225946 0.06350444 0.0379371  0.01498908 0.56130993]\n",
            " [0.31228125 0.06286604 0.03738579 0.01492365 0.5725432 ]\n",
            " [0.3195468  0.06332365 0.03778384 0.01497512 0.5643706 ]\n",
            " [0.31244802 0.06280346 0.03737674 0.01490383 0.572468  ]\n",
            " [0.31244594 0.06280436 0.03737646 0.01490295 0.57247025]\n",
            " [0.31240413 0.06282271 0.0373793  0.01490848 0.5724854 ]\n",
            " [0.31982762 0.06331099 0.03779098 0.01496124 0.56410915]\n",
            " [0.32221457 0.06352147 0.03794067 0.01499644 0.5613268 ]\n",
            " [0.31232986 0.06285188 0.03738488 0.01491948 0.57251394]\n",
            " [0.32227126 0.06350178 0.03793683 0.01498769 0.5613024 ]\n",
            " [0.31247452 0.06279667 0.03737602 0.01490078 0.572452  ]\n",
            " [0.32224795 0.06350562 0.03793681 0.01498946 0.5613201 ]\n",
            " [0.32222033 0.06350646 0.03793707 0.01499407 0.56134206]\n",
            " [0.31980008 0.06331787 0.03779225 0.01496664 0.5641232 ]\n",
            " [0.31242764 0.06281364 0.03737838 0.01490608 0.5724742 ]\n",
            " [0.31250364 0.06278321 0.03737401 0.0148965  0.57244265]\n",
            " [0.32225236 0.06350653 0.03793783 0.01499114 0.5613121 ]\n",
            " [0.31242767 0.0628138  0.03737835 0.01490616 0.572474  ]\n",
            " [0.32224908 0.0635076  0.037938   0.01499152 0.5613138 ]\n",
            " [0.3123324  0.062849   0.03738358 0.01491807 0.572517  ]\n",
            " [0.32223973 0.06351029 0.03793826 0.01499246 0.56131923]\n",
            " [0.31248787 0.0627893  0.0373745  0.01489758 0.5724508 ]\n",
            " [0.31246716 0.06279816 0.03737567 0.01490038 0.57245857]\n",
            " [0.32227975 0.06349697 0.03793598 0.01498653 0.5613007 ]\n",
            " [0.31241623 0.06281798 0.03737891 0.0149074  0.57247955]\n",
            " [0.32228762 0.06349454 0.03793623 0.01498701 0.56129456]\n",
            " [0.31247538 0.06279394 0.0373753  0.01489988 0.57245547]\n",
            " [0.31246206 0.06279956 0.03737588 0.01490095 0.5724616 ]\n",
            " [0.31235248 0.06284067 0.03738183 0.01491482 0.5725102 ]\n",
            " [0.3173178  0.06314296 0.03764997 0.01494383 0.56694543]\n",
            " [0.3222202  0.0635196  0.03793858 0.01499399 0.56132776]\n",
            " [0.31241935 0.06281555 0.03737904 0.01490827 0.57247776]\n",
            " [0.31982034 0.06330176 0.03778888 0.01496192 0.5641271 ]\n",
            " [0.31477442 0.06300066 0.03751618 0.01493499 0.56977373]\n",
            " [0.31247565 0.06279255 0.037375   0.01490026 0.5724565 ]\n",
            " [0.3222745  0.06349862 0.03793658 0.01498828 0.5613019 ]\n",
            " [0.3123635  0.06284386 0.03738404 0.01491603 0.57249254]\n",
            " [0.31247413 0.0627946  0.03737578 0.01490079 0.57245463]\n",
            " [0.3124539  0.06280246 0.03737615 0.0149018  0.57246584]\n",
            " [0.31238866 0.06282838 0.03738044 0.01491088 0.5724917 ]\n",
            " [0.31239387 0.06282628 0.03738103 0.01491317 0.5724856 ]\n",
            " [0.32229364 0.0634923  0.03793588 0.01498621 0.561292  ]]\n",
            "<class 'numpy.ndarray'>\n",
            "yhat:  [[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]]\n",
            "<class 'numpy.ndarray'>\n",
            "Stacked Test Accuracy: 0.943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.metrics.mean_squared_error(testlab,yhat))\n",
        "print(sklearn.metrics.mean_absolute_error(testlab,yhat))\n",
        "print(sklearn.metrics.f1_score(testlab,yhat,average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9qbNX3w4P_Y",
        "outputId": "7cb0a332-659f-44d7-90cd-02a70bfcdd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12295081967213115\n",
            "0.07377049180327869\n",
            "0.9358167218245722\n"
          ]
        }
      ]
    }
  ]
}