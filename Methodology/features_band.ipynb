{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qejL93IwpgnO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('features_band.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "U1CZLVOinTem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Gaussian Process\",\n",
        "         \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n",
        "         \"Naive Bayes\"]\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(5),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
        "    DecisionTreeClassifier(max_depth=3),\n",
        "    RandomForestClassifier(max_depth=3, n_estimators=10),\n",
        "    AdaBoostClassifier(),\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "svH6HWWVo49s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpc_values = df['CPC'].values\n",
        "\n",
        "# Normalize EEG data (110 columns)\n",
        "eeg_data = df.iloc[:, 7:81].values  # these columns are EEG data\n",
        "scaler_eeg = MinMaxScaler()\n",
        "eeg_data_normalized = scaler_eeg.fit_transform(eeg_data)\n",
        "static_features = df.iloc[:, :7].values\n",
        "# Normalize static features\n",
        "scaler_static = MinMaxScaler()\n",
        "static_features_normalized = scaler_static.fit_transform(static_features)\n",
        "# Combine normalized static features, normalized EEG data, and CPC values\n",
        "normalized_data = np.concatenate((static_features_normalized, eeg_data_normalized), axis=1)\n",
        "\n",
        "# Convert CPC values to one-hot encoded labels\n",
        "one_hot_labels = to_categorical(cpc_values - 1, num_classes=5)  # Assuming CPC values start from 1\n",
        "# Train-test split\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    normalized_data, one_hot_labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "gc-sBNFMnq8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testlab=[]\n",
        "for i in test_labels.tolist():\n",
        "  testlab.append([i.index(1.0)+1])\n",
        "trainlab=[]\n",
        "for i in train_labels.tolist():\n",
        "  trainlab.append([i.index(1.0)+1])"
      ],
      "metadata": {
        "id": "Sxnnd3D-n-uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "# Define model\n",
        "model1 = Sequential()\n",
        "model1.add(LSTM(units=128, input_shape=(81, 1), return_sequences=True))  # Input shape should match the number of EEG columns (117)\n",
        "model1.add(Dropout(0.3))  # Add dropout layer with a dropout rate of 30%\n",
        "model1.add(LSTM(units=64, return_sequences=True))\n",
        "model1.add(LSTM(units=32))\n",
        "model1.add(Dense(units=16, activation='relu'))\n",
        "model1.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "t799agGCoLzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#features_band\n",
        "#for 81 columns-features_dt\n",
        "model1.fit(x=train_data, y=train_labels, epochs=50, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwxrXSzuSp9b",
        "outputId": "14187d17-246e-49b2-afda-f7fec0828669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "13/13 [==============================] - 12s 444ms/step - loss: 1.4320 - accuracy: 0.5530 - val_loss: 1.0458 - val_accuracy: 0.6082\n",
            "Epoch 2/50\n",
            "13/13 [==============================] - 3s 235ms/step - loss: 1.0604 - accuracy: 0.5788 - val_loss: 0.9958 - val_accuracy: 0.6082\n",
            "Epoch 3/50\n",
            "13/13 [==============================] - 3s 240ms/step - loss: 1.0414 - accuracy: 0.5788 - val_loss: 0.9874 - val_accuracy: 0.6082\n",
            "Epoch 4/50\n",
            "13/13 [==============================] - 3s 238ms/step - loss: 1.0381 - accuracy: 0.5788 - val_loss: 0.9795 - val_accuracy: 0.6082\n",
            "Epoch 5/50\n",
            "13/13 [==============================] - 4s 335ms/step - loss: 1.0361 - accuracy: 0.5788 - val_loss: 0.9745 - val_accuracy: 0.6082\n",
            "Epoch 6/50\n",
            "13/13 [==============================] - 3s 243ms/step - loss: 1.0368 - accuracy: 0.5788 - val_loss: 0.9722 - val_accuracy: 0.6082\n",
            "Epoch 7/50\n",
            "13/13 [==============================] - 3s 236ms/step - loss: 1.0339 - accuracy: 0.5788 - val_loss: 0.9717 - val_accuracy: 0.6082\n",
            "Epoch 8/50\n",
            "13/13 [==============================] - 3s 270ms/step - loss: 1.0340 - accuracy: 0.5788 - val_loss: 0.9709 - val_accuracy: 0.6082\n",
            "Epoch 9/50\n",
            "13/13 [==============================] - 4s 297ms/step - loss: 1.0389 - accuracy: 0.5788 - val_loss: 0.9754 - val_accuracy: 0.6082\n",
            "Epoch 10/50\n",
            "13/13 [==============================] - 3s 240ms/step - loss: 1.0371 - accuracy: 0.5788 - val_loss: 0.9679 - val_accuracy: 0.6082\n",
            "Epoch 11/50\n",
            "13/13 [==============================] - 3s 233ms/step - loss: 1.0348 - accuracy: 0.5788 - val_loss: 0.9762 - val_accuracy: 0.6082\n",
            "Epoch 12/50\n",
            "13/13 [==============================] - 4s 306ms/step - loss: 1.0354 - accuracy: 0.5788 - val_loss: 0.9720 - val_accuracy: 0.6082\n",
            "Epoch 13/50\n",
            "13/13 [==============================] - 4s 263ms/step - loss: 1.0333 - accuracy: 0.5788 - val_loss: 0.9668 - val_accuracy: 0.6082\n",
            "Epoch 14/50\n",
            "13/13 [==============================] - 3s 233ms/step - loss: 1.0329 - accuracy: 0.5788 - val_loss: 0.9640 - val_accuracy: 0.6082\n",
            "Epoch 15/50\n",
            "13/13 [==============================] - 3s 237ms/step - loss: 1.0304 - accuracy: 0.5788 - val_loss: 0.9685 - val_accuracy: 0.6082\n",
            "Epoch 16/50\n",
            "13/13 [==============================] - 4s 309ms/step - loss: 1.0341 - accuracy: 0.5788 - val_loss: 0.9721 - val_accuracy: 0.6082\n",
            "Epoch 17/50\n",
            "13/13 [==============================] - 3s 244ms/step - loss: 1.0285 - accuracy: 0.5788 - val_loss: 0.9639 - val_accuracy: 0.6082\n",
            "Epoch 18/50\n",
            "13/13 [==============================] - 3s 240ms/step - loss: 1.0278 - accuracy: 0.5788 - val_loss: 0.9636 - val_accuracy: 0.6082\n",
            "Epoch 19/50\n",
            "13/13 [==============================] - 3s 231ms/step - loss: 1.0256 - accuracy: 0.5788 - val_loss: 0.9595 - val_accuracy: 0.6082\n",
            "Epoch 20/50\n",
            "13/13 [==============================] - 4s 343ms/step - loss: 1.0215 - accuracy: 0.5788 - val_loss: 0.9548 - val_accuracy: 0.6082\n",
            "Epoch 21/50\n",
            "13/13 [==============================] - 3s 234ms/step - loss: 1.0144 - accuracy: 0.5788 - val_loss: 0.9448 - val_accuracy: 0.6082\n",
            "Epoch 22/50\n",
            "13/13 [==============================] - 3s 235ms/step - loss: 0.9990 - accuracy: 0.5788 - val_loss: 0.9203 - val_accuracy: 0.6082\n",
            "Epoch 23/50\n",
            "13/13 [==============================] - 3s 227ms/step - loss: 0.9793 - accuracy: 0.5788 - val_loss: 0.8850 - val_accuracy: 0.6082\n",
            "Epoch 24/50\n",
            "13/13 [==============================] - 4s 340ms/step - loss: 0.9331 - accuracy: 0.5788 - val_loss: 0.8386 - val_accuracy: 0.6082\n",
            "Epoch 25/50\n",
            "13/13 [==============================] - 3s 221ms/step - loss: 0.8876 - accuracy: 0.5788 - val_loss: 0.7559 - val_accuracy: 0.6082\n",
            "Epoch 26/50\n",
            "13/13 [==============================] - 3s 231ms/step - loss: 0.7963 - accuracy: 0.7623 - val_loss: 0.6820 - val_accuracy: 0.8969\n",
            "Epoch 27/50\n",
            "13/13 [==============================] - 3s 229ms/step - loss: 0.6965 - accuracy: 0.8734 - val_loss: 0.5605 - val_accuracy: 0.8969\n",
            "Epoch 28/50\n",
            "13/13 [==============================] - 4s 344ms/step - loss: 0.6297 - accuracy: 0.8863 - val_loss: 0.4694 - val_accuracy: 0.8969\n",
            "Epoch 29/50\n",
            "13/13 [==============================] - 3s 230ms/step - loss: 0.5197 - accuracy: 0.8863 - val_loss: 0.4000 - val_accuracy: 0.8969\n",
            "Epoch 30/50\n",
            "13/13 [==============================] - 3s 241ms/step - loss: 0.4597 - accuracy: 0.8863 - val_loss: 0.3510 - val_accuracy: 0.8969\n",
            "Epoch 31/50\n",
            "13/13 [==============================] - 3s 242ms/step - loss: 0.4172 - accuracy: 0.8889 - val_loss: 0.3188 - val_accuracy: 0.8969\n",
            "Epoch 32/50\n",
            "13/13 [==============================] - 5s 360ms/step - loss: 0.3809 - accuracy: 0.8889 - val_loss: 0.3069 - val_accuracy: 0.8969\n",
            "Epoch 33/50\n",
            "13/13 [==============================] - 3s 233ms/step - loss: 0.3689 - accuracy: 0.8889 - val_loss: 0.2940 - val_accuracy: 0.8969\n",
            "Epoch 34/50\n",
            "13/13 [==============================] - 3s 235ms/step - loss: 0.3430 - accuracy: 0.8889 - val_loss: 0.2654 - val_accuracy: 0.8969\n",
            "Epoch 35/50\n",
            "13/13 [==============================] - 3s 242ms/step - loss: 0.3375 - accuracy: 0.8889 - val_loss: 0.2497 - val_accuracy: 0.8969\n",
            "Epoch 36/50\n",
            "13/13 [==============================] - 5s 353ms/step - loss: 0.3159 - accuracy: 0.8889 - val_loss: 0.2360 - val_accuracy: 0.8969\n",
            "Epoch 37/50\n",
            "13/13 [==============================] - 3s 242ms/step - loss: 0.3630 - accuracy: 0.8863 - val_loss: 0.2327 - val_accuracy: 0.8969\n",
            "Epoch 38/50\n",
            "13/13 [==============================] - 3s 238ms/step - loss: 0.3354 - accuracy: 0.8889 - val_loss: 0.3136 - val_accuracy: 0.8969\n",
            "Epoch 39/50\n",
            "13/13 [==============================] - 3s 272ms/step - loss: 0.3446 - accuracy: 0.8941 - val_loss: 0.2910 - val_accuracy: 0.8969\n",
            "Epoch 40/50\n",
            "13/13 [==============================] - 4s 297ms/step - loss: 0.2998 - accuracy: 0.8941 - val_loss: 0.2184 - val_accuracy: 0.8969\n",
            "Epoch 41/50\n",
            "13/13 [==============================] - 3s 225ms/step - loss: 0.3030 - accuracy: 0.8941 - val_loss: 0.2138 - val_accuracy: 0.9175\n",
            "Epoch 42/50\n",
            "13/13 [==============================] - 3s 238ms/step - loss: 0.2581 - accuracy: 0.9070 - val_loss: 0.1916 - val_accuracy: 0.9175\n",
            "Epoch 43/50\n",
            "13/13 [==============================] - 4s 276ms/step - loss: 0.2622 - accuracy: 0.8992 - val_loss: 0.2437 - val_accuracy: 0.9175\n",
            "Epoch 44/50\n",
            "13/13 [==============================] - 4s 291ms/step - loss: 0.2764 - accuracy: 0.9044 - val_loss: 0.1861 - val_accuracy: 0.9175\n",
            "Epoch 45/50\n",
            "13/13 [==============================] - 3s 235ms/step - loss: 0.2303 - accuracy: 0.9147 - val_loss: 0.1607 - val_accuracy: 0.9175\n",
            "Epoch 46/50\n",
            "13/13 [==============================] - 3s 227ms/step - loss: 0.2309 - accuracy: 0.9147 - val_loss: 0.1483 - val_accuracy: 0.9175\n",
            "Epoch 47/50\n",
            "13/13 [==============================] - 3s 266ms/step - loss: 0.2089 - accuracy: 0.9354 - val_loss: 0.1403 - val_accuracy: 0.9175\n",
            "Epoch 48/50\n",
            "13/13 [==============================] - 4s 273ms/step - loss: 0.2128 - accuracy: 0.9225 - val_loss: 0.1318 - val_accuracy: 0.9381\n",
            "Epoch 49/50\n",
            "13/13 [==============================] - 3s 231ms/step - loss: 0.2088 - accuracy: 0.9251 - val_loss: 0.1388 - val_accuracy: 0.9175\n",
            "Epoch 50/50\n",
            "13/13 [==============================] - 3s 226ms/step - loss: 0.1958 - accuracy: 0.9561 - val_loss: 0.1355 - val_accuracy: 0.9175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe5dbc754e0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model1.evaluate(x=test_data, y=test_labels)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT_O-7IMSsLL",
        "outputId": "b7e18ee0-6afd-4e85-cdcc-8cf786e76949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 67ms/step - loss: 0.2688 - accuracy: 0.8607\n",
            "Test Loss: 0.2687661051750183, Test Accuracy: 0.8606557250022888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.spatial.distance import euclidean\n",
        "from math import exp\n",
        "\n",
        "\n",
        "\n",
        "gem_matrix_train = np.zeros((train_data.shape[0], train_data.shape[0]))\n",
        "for i in range(train_data.shape[0]):\n",
        "    for j in range(i, train_data.shape[0]):\n",
        "        dtw_distance = np.sqrt(np.sum(np.square(train_data[i] - train_data[j])))\n",
        "        gem_matrix_train[i, j] = exp(-dtw_distance ** 2)\n",
        "        gem_matrix_train[j, i] = gem_matrix_train[i, j]\n",
        "\n",
        "gem_matrix_test = np.zeros((test_data.shape[0], train_data.shape[0]))\n",
        "for i in range(test_data.shape[0]):\n",
        "    for j in range(train_data.shape[0]):\n",
        "        dtw_distance = np.sqrt(np.sum(np.square(test_data[i] - train_data[j])))\n",
        "        gem_matrix_test[i, j] = exp(-dtw_distance ** 2)\n",
        "\n",
        "clf = SVC(kernel='precomputed')\n",
        "clf.fit(gem_matrix_train, trainlab)\n",
        "y_pred = clf.predict(gem_matrix_test)\n",
        "\n",
        "accuracy = accuracy_score(testlab, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Njx5qjUx-j",
        "outputId": "bbb80747-eb29-4f92-ff23-d6f1dd886c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#81 columns\n",
        "# stacked generalization with linear meta model on blobs dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from numpy import dstack\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "members=[model1,clf]\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX):\n",
        "  stackX = None\n",
        "  for model in members:\n",
        "    # make prediction\n",
        "    if(model!=clf):\n",
        "      yhat = model.predict(inputX)\n",
        "      print('yhat: ',yhat)\n",
        "      print(type(yhat))\n",
        "    else:\n",
        "      gem_matrix_test = np.zeros((inputX.shape[0], train_data.shape[0]))\n",
        "      for i in range(inputX.shape[0]):\n",
        "          for j in range(train_data.shape[0]):\n",
        "              dtw_distance = np.sqrt(np.sum(np.square(inputX[i] - train_data[j])))\n",
        "              gem_matrix_test[i, j] = exp(-dtw_distance ** 2)\n",
        "      yhat = model.predict(gem_matrix_test)\n",
        "      yhat1=[]\n",
        "      for i in yhat.tolist():\n",
        "        l=[0.0,0.0,0.0,0.0,0.0]\n",
        "        l[i-1]=1.0\n",
        "        yhat1.append(l)\n",
        "      yhat=np.array(yhat1)\n",
        "      print('yhat: ',yhat)\n",
        "      print(type(yhat))\n",
        "    # stack predictions into [rows, members, probabilities]\n",
        "    if stackX is None:\n",
        "      stackX = yhat\n",
        "    else:\n",
        "      stackX = dstack((stackX, yhat))\n",
        "\t# flatten predictions to [rows, members x probabilities]\n",
        "  stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "  return stackX\n",
        "\n",
        "#[1 0 0 0 0]\n",
        "#[0 1 0 0 0]\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model(members, inputX, inputy):\n",
        "  # create dataset using ensemble\n",
        "  stackedX = stacked_dataset(members, inputX)\n",
        "  print('stacked: ',stackedX)\n",
        "  # fit standalone model\n",
        "  #model = LogisticRegression()\n",
        "  #model=clf\n",
        "  #model=classifiers[0]\n",
        "  model=knn1\n",
        "  #model=rfmodel ----->give 100 percent accuracy\n",
        "  model.fit(stackedX, inputy)\n",
        "  return model\n",
        "\n",
        "# make a prediction with the stacked model\n",
        "def stacked_prediction(members, model, inputX):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# make a prediction\n",
        "\tyhat = model.predict(stackedX)\n",
        "\treturn yhat\n",
        "\n",
        "# fit stacked model using the ensemble\n",
        "model = fit_stacked_model(members, test_data, testlab)\n",
        "# evaluate model on test set\n",
        "yhat = stacked_prediction(members, model, test_data)\n",
        "acc = accuracy_score(testlab, yhat)\n",
        "print('Stacked Test Accuracy: %.3f' % acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxt9sbD_dBQF",
        "outputId": "4b911ea4-fd7e-40ec-b69f-be9316c48aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 3s 102ms/step\n",
            "yhat:  [[9.68891982e-05 3.35637340e-03 5.89827728e-03 1.16152111e-02\n",
            "  9.79033113e-01]\n",
            " [7.65519799e-05 2.53086118e-03 4.42599319e-03 8.94282386e-03\n",
            "  9.84023690e-01]\n",
            " [2.06928793e-02 3.10279101e-01 5.80528855e-01 5.42895421e-02\n",
            "  3.42096351e-02]\n",
            " [9.67522792e-05 3.35021294e-03 5.88381151e-03 1.15938159e-02\n",
            "  9.79075253e-01]\n",
            " [8.92506287e-05 2.37224414e-03 3.69793014e-03 7.96466321e-03\n",
            "  9.85875905e-01]\n",
            " [2.17432663e-01 6.18489385e-01 1.48567051e-01 1.05923982e-02\n",
            "  4.91853850e-03]\n",
            " [3.18944663e-01 5.74841321e-01 9.04377252e-02 9.25908703e-03\n",
            "  6.51714532e-03]\n",
            " [9.57839075e-05 2.43442948e-03 3.72009748e-03 8.07298627e-03\n",
            "  9.85676587e-01]\n",
            " [9.94691253e-01 5.00813406e-03 2.59535998e-04 3.81116333e-05\n",
            "  3.06111588e-06]\n",
            " [9.67617816e-05 3.35063459e-03 5.88479964e-03 1.15952808e-02\n",
            "  9.79072452e-01]\n",
            " [9.67900560e-05 3.35182971e-03 5.88754099e-03 1.15993768e-02\n",
            "  9.79064405e-01]\n",
            " [9.67689484e-05 3.35095939e-03 5.88556053e-03 1.15964040e-02\n",
            "  9.79070246e-01]\n",
            " [9.68187233e-05 3.35324113e-03 5.89095289e-03 1.16043622e-02\n",
            "  9.79054630e-01]\n",
            " [9.24400229e-05 3.18368524e-03 5.53528359e-03 1.10394545e-02\n",
            "  9.80149031e-01]\n",
            " [9.67840388e-05 3.35158105e-03 5.88697568e-03 1.15985228e-02\n",
            "  9.79066074e-01]\n",
            " [2.31735525e-03 7.67669082e-02 2.23789826e-01 8.75949115e-02\n",
            "  6.09530985e-01]\n",
            " [2.52573818e-01 6.12985611e-01 1.21775776e-01 8.67995899e-03\n",
            "  3.98478471e-03]\n",
            " [2.12316349e-01 6.18738174e-01 1.52960554e-01 1.09093385e-02\n",
            "  5.07562421e-03]\n",
            " [9.67631277e-05 3.35070025e-03 5.88495657e-03 1.15955081e-02\n",
            "  9.79072034e-01]\n",
            " [9.67529995e-05 3.35025229e-03 5.88390557e-03 1.15939509e-02\n",
            "  9.79075074e-01]\n",
            " [9.94596839e-01 5.09622646e-03 2.65096314e-04 3.88125882e-05\n",
            "  3.13228747e-06]\n",
            " [9.67829692e-05 3.35141597e-03 5.88650582e-03 1.15978792e-02\n",
            "  9.79067326e-01]\n",
            " [9.67645828e-05 3.35073634e-03 5.88502269e-03 1.15956152e-02\n",
            "  9.79071796e-01]\n",
            " [9.05849229e-05 3.10889864e-03 5.37530798e-03 1.07870596e-02\n",
            "  9.80638027e-01]\n",
            " [9.67519227e-05 3.35020362e-03 5.88379288e-03 1.15937833e-02\n",
            "  9.79075372e-01]\n",
            " [9.94692206e-01 5.00717433e-03 2.59490480e-04 3.81063219e-05\n",
            "  3.06050879e-06]\n",
            " [2.52901256e-01 6.12907708e-01 1.21551000e-01 8.66337959e-03\n",
            "  3.97656485e-03]\n",
            " [8.58505227e-05 2.92738969e-03 5.01164189e-03 1.01856682e-02\n",
            "  9.81789351e-01]\n",
            " [2.06806343e-02 3.10198903e-01 5.80594480e-01 5.43031991e-02\n",
            "  3.42227146e-02]\n",
            " [9.84449625e-01 1.47224711e-02 7.24155223e-04 9.30392052e-05\n",
            "  1.06201878e-05]\n",
            " [9.67784872e-05 3.35134077e-03 5.88641595e-03 1.15976976e-02\n",
            "  9.79067802e-01]\n",
            " [9.67573287e-05 3.35043063e-03 5.88431489e-03 1.15945637e-02\n",
            "  9.79074001e-01]\n",
            " [9.57855373e-05 2.43446836e-03 3.72016593e-03 8.07310827e-03\n",
            "  9.85676348e-01]\n",
            " [7.76997331e-05 2.63598864e-03 4.55968222e-03 9.28209629e-03\n",
            "  9.83444631e-01]\n",
            " [9.67565938e-05 3.35040712e-03 5.88426506e-03 1.15944874e-02\n",
            "  9.79074121e-01]\n",
            " [9.35356002e-05 3.22383293e-03 5.61475148e-03 1.11707132e-02\n",
            "  9.79897082e-01]\n",
            " [3.84033145e-03 1.19414009e-01 3.61824930e-01 1.03273652e-01\n",
            "  4.11647081e-01]\n",
            " [2.22802624e-01 6.18071258e-01 1.44095302e-01 1.02709150e-02\n",
            "  4.75995569e-03]\n",
            " [9.67734450e-05 3.35113774e-03 5.88596612e-03 1.15970103e-02\n",
            "  9.79069114e-01]\n",
            " [9.06446148e-05 3.11153498e-03 5.38140582e-03 1.07962368e-02\n",
            "  9.80620027e-01]\n",
            " [9.67540836e-05 3.35029769e-03 5.88401081e-03 1.15941139e-02\n",
            "  9.79074836e-01]\n",
            " [2.52534568e-01 6.12994969e-01 1.21802717e-01 8.68194923e-03\n",
            "  3.98578076e-03]\n",
            " [9.67567830e-05 3.35041201e-03 5.88427344e-03 1.15945041e-02\n",
            "  9.79074121e-01]\n",
            " [9.67583837e-05 3.35048162e-03 5.88443549e-03 1.15947463e-02\n",
            "  9.79073524e-01]\n",
            " [9.94691253e-01 5.00811543e-03 2.59528577e-04 3.81103928e-05\n",
            "  3.06101106e-06]\n",
            " [9.69045213e-05 3.35651333e-03 5.89822559e-03 1.16153639e-02\n",
            "  9.79032874e-01]\n",
            " [9.94106889e-01 5.55269653e-03 2.94260826e-04 4.24735481e-05\n",
            "  3.50933806e-06]\n",
            " [9.94600058e-01 5.09321596e-03 2.64913455e-04 3.87888103e-05\n",
            "  3.12981024e-06]\n",
            " [2.06839386e-02 3.10219914e-01 5.80577433e-01 5.42996004e-02\n",
            "  3.42191905e-02]\n",
            " [9.94690299e-01 5.00907283e-03 2.59586988e-04 3.81179561e-05\n",
            "  3.06179618e-06]\n",
            " [9.94690537e-01 5.00882091e-03 2.59574212e-04 3.81164391e-05\n",
            "  3.06162769e-06]\n",
            " [9.67630403e-05 3.35070025e-03 5.88495657e-03 1.15955081e-02\n",
            "  9.79072034e-01]\n",
            " [9.67515662e-05 3.35019128e-03 5.88376820e-03 1.15937460e-02\n",
            "  9.79075491e-01]\n",
            " [9.57835582e-05 2.43442412e-03 3.72009119e-03 8.07297602e-03\n",
            "  9.85676706e-01]\n",
            " [8.75443802e-05 2.35973322e-03 3.70277278e-03 7.95166101e-03\n",
            "  9.85898256e-01]\n",
            " [9.94692087e-01 5.00719529e-03 2.59477063e-04 3.81038844e-05\n",
            "  3.06031870e-06]\n",
            " [9.83970940e-01 1.51290065e-02 7.87586323e-04 1.01042038e-04\n",
            "  1.15398416e-05]\n",
            " [9.67754095e-05 3.35124554e-03 5.88622876e-03 1.15973940e-02\n",
            "  9.79068398e-01]\n",
            " [9.94690537e-01 5.00875851e-03 2.59568507e-04 3.81156024e-05\n",
            "  3.06155175e-06]\n",
            " [9.67646629e-05 3.35075497e-03 5.88506972e-03 1.15956804e-02\n",
            "  9.79071677e-01]\n",
            " [8.68875286e-05 2.96678348e-03 5.08867577e-03 1.03155132e-02\n",
            "  9.81542230e-01]\n",
            " [9.68351160e-05 3.35386954e-03 5.89234894e-03 1.16064753e-02\n",
            "  9.79050398e-01]\n",
            " [9.20154416e-05 3.16448230e-03 5.49040455e-03 1.09724244e-02\n",
            "  9.80280578e-01]\n",
            " [9.67537126e-05 3.35028162e-03 5.88397449e-03 1.15940543e-02\n",
            "  9.79074836e-01]\n",
            " [9.68450622e-05 3.35444417e-03 5.89379156e-03 1.16085475e-02\n",
            "  9.79046345e-01]\n",
            " [9.94690180e-01 5.00912452e-03 2.59590423e-04 3.81184582e-05\n",
            "  3.06184825e-06]\n",
            " [9.36085926e-05 3.22701945e-03 5.62209170e-03 1.11817336e-02\n",
            "  9.79875505e-01]\n",
            " [2.53175348e-01 6.12851620e-01 1.21355705e-01 8.64832290e-03\n",
            "  3.96898761e-03]\n",
            " [9.94690299e-01 5.00906305e-03 2.59587745e-04 3.81181744e-05\n",
            "  3.06181096e-06]\n",
            " [9.94690299e-01 5.00898669e-03 2.59581808e-04 3.81173377e-05\n",
            "  3.06172910e-06]\n",
            " [7.67064266e-05 2.59192172e-03 4.54552518e-03 9.16794688e-03\n",
            "  9.83617902e-01]\n",
            " [9.67919405e-05 3.35202902e-03 5.88809652e-03 1.16001386e-02\n",
            "  9.79062915e-01]\n",
            " [2.53325760e-01 6.12816274e-01 1.21252120e-01 8.64066835e-03\n",
            "  3.96519853e-03]\n",
            " [9.94691014e-01 5.00832452e-03 2.59548076e-04 3.81132959e-05\n",
            "  3.06128459e-06]\n",
            " [9.94690299e-01 5.00896294e-03 2.59582041e-04 3.81174104e-05\n",
            "  3.06173206e-06]\n",
            " [9.94690180e-01 5.00911986e-03 2.59592169e-04 3.81187892e-05\n",
            "  3.06187189e-06]\n",
            " [9.67857122e-05 3.35158804e-03 5.88694541e-03 1.15985069e-02\n",
            "  9.79066193e-01]\n",
            " [9.94691253e-01 5.00802230e-03 2.59525375e-04 3.81100690e-05\n",
            "  3.06097013e-06]\n",
            " [8.37344705e-05 2.34634406e-03 3.75366001e-03 7.98246637e-03\n",
            "  9.85833824e-01]\n",
            " [9.67649976e-05 3.35079525e-03 5.88518241e-03 1.15958424e-02\n",
            "  9.79071319e-01]\n",
            " [9.94693041e-01 5.00630029e-03 2.59428081e-04 3.80978199e-05\n",
            "  3.05967092e-06]\n",
            " [9.67729211e-05 3.35110817e-03 5.88589208e-03 1.15969079e-02\n",
            "  9.79069352e-01]\n",
            " [2.52590925e-01 6.12982035e-01 1.21763639e-01 8.67904350e-03\n",
            "  3.98433208e-03]\n",
            " [9.67541710e-05 3.35029606e-03 5.88400010e-03 1.15940981e-02\n",
            "  9.79074836e-01]\n",
            " [9.67918968e-05 3.35197640e-03 5.88793913e-03 1.15999291e-02\n",
            "  9.79063392e-01]\n",
            " [9.67784435e-05 3.35138571e-03 5.88656170e-03 1.15978848e-02\n",
            "  9.79067326e-01]\n",
            " [1.73721388e-01 6.15888774e-01 1.90131694e-01 1.37289744e-02\n",
            "  6.52930886e-03]\n",
            " [9.94690061e-01 5.00923162e-03 2.59596593e-04 3.81192185e-05\n",
            "  3.06192692e-06]\n",
            " [9.67547967e-05 3.35033028e-03 5.88408858e-03 1.15942229e-02\n",
            "  9.79074597e-01]\n",
            " [9.94690418e-01 5.00887539e-03 2.59575143e-04 3.81165082e-05\n",
            "  3.06164475e-06]\n",
            " [9.51789698e-05 3.28904949e-03 5.75417560e-03 1.13897100e-02\n",
            "  9.79471922e-01]\n",
            " [9.94690537e-01 5.00883302e-03 2.59574212e-04 3.81164391e-05\n",
            "  3.06163042e-06]\n",
            " [9.94690180e-01 5.00917481e-03 2.59594904e-04 3.81191494e-05\n",
            "  3.06191259e-06]\n",
            " [2.46193051e-01 6.14449620e-01 1.26225129e-01 8.99512786e-03\n",
            "  4.13699355e-03]\n",
            " [9.67533706e-05 3.35026183e-03 5.88392001e-03 1.15939798e-02\n",
            "  9.79075074e-01]\n",
            " [9.67559608e-05 3.35038197e-03 5.88421244e-03 1.15944063e-02\n",
            "  9.79074240e-01]\n",
            " [9.77316797e-01 2.14494504e-02 1.08508661e-03 1.31806271e-04\n",
            "  1.68699953e-05]\n",
            " [9.67862288e-05 3.35162180e-03 5.88703854e-03 1.15986364e-02\n",
            "  9.79065835e-01]\n",
            " [9.77316797e-01 2.14494411e-02 1.08508661e-03 1.31806140e-04\n",
            "  1.68699953e-05]\n",
            " [9.05853594e-05 3.10892123e-03 5.37536200e-03 1.07871415e-02\n",
            "  9.80638027e-01]\n",
            " [9.94690776e-01 5.00858342e-03 2.59559631e-04 3.81145910e-05\n",
            "  3.06143261e-06]\n",
            " [9.67840388e-05 3.35158431e-03 5.88698359e-03 1.15985395e-02\n",
            "  9.79066074e-01]\n",
            " [9.07734939e-05 3.11709568e-03 5.39418310e-03 1.08155105e-02\n",
            "  9.80582416e-01]\n",
            " [9.94692087e-01 5.00720739e-03 2.59476103e-04 3.81037389e-05\n",
            "  3.06031575e-06]\n",
            " [8.48628770e-05 2.89038336e-03 4.94126603e-03 1.00646159e-02\n",
            "  9.82018769e-01]\n",
            " [9.94690418e-01 5.00894943e-03 2.59579130e-04 3.81169411e-05\n",
            "  3.06169136e-06]\n",
            " [8.69774813e-05 2.97074276e-03 5.09779295e-03 1.03293434e-02\n",
            "  9.81515050e-01]\n",
            " [9.67524538e-05 3.35022993e-03 5.88385342e-03 1.15938755e-02\n",
            "  9.79075134e-01]\n",
            " [9.21304672e-05 3.16968630e-03 5.50255785e-03 1.09905945e-02\n",
            "  9.80245054e-01]\n",
            " [1.30371759e-02 2.00921550e-01 2.97842264e-01 8.18905681e-02\n",
            "  4.06308502e-01]\n",
            " [9.93164003e-01 6.43038657e-03 3.51678464e-04 4.97480905e-05\n",
            "  4.28636395e-06]\n",
            " [7.94739535e-05 2.39140773e-03 3.97623423e-03 8.27368349e-03\n",
            "  9.85279202e-01]\n",
            " [1.68738469e-01 6.15441263e-01 1.94856733e-01 1.41761284e-02\n",
            "  6.78743748e-03]\n",
            " [5.12739876e-04 1.52426092e-02 3.27264331e-02 2.98735350e-02\n",
            "  9.21644688e-01]\n",
            " [8.15447347e-05 2.76786997e-03 4.72019706e-03 9.66904685e-03\n",
            "  9.82761323e-01]\n",
            " [9.94310319e-01 5.36338240e-03 2.82113324e-04 4.09503409e-05\n",
            "  3.35126811e-06]\n",
            " [9.68213499e-05 3.35320877e-03 5.89076430e-03 1.16041554e-02\n",
            "  9.79054987e-01]\n",
            " [9.67653614e-05 3.35080340e-03 5.88519406e-03 1.15958648e-02\n",
            "  9.79071319e-01]\n",
            " [9.57837328e-05 2.43442715e-03 3.72009748e-03 8.07298627e-03\n",
            "  9.85676587e-01]\n",
            " [9.67528176e-05 3.35024414e-03 5.88388927e-03 1.15939295e-02\n",
            "  9.79075074e-01]\n",
            " [9.35461649e-05 3.22412490e-03 5.61520876e-03 1.11715700e-02\n",
            "  9.79895592e-01]\n",
            " [9.94690955e-01 5.00829984e-03 2.59540160e-04 3.81118734e-05\n",
            "  3.06116749e-06]]\n",
            "<class 'numpy.ndarray'>\n",
            "yhat:  [[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]]\n",
            "<class 'numpy.ndarray'>\n",
            "stacked:  [[9.68891982e-05 0.00000000e+00 3.35637340e-03 ... 0.00000000e+00\n",
            "  9.79033113e-01 1.00000000e+00]\n",
            " [7.65519799e-05 0.00000000e+00 2.53086118e-03 ... 0.00000000e+00\n",
            "  9.84023690e-01 1.00000000e+00]\n",
            " [2.06928793e-02 0.00000000e+00 3.10279101e-01 ... 0.00000000e+00\n",
            "  3.42096351e-02 0.00000000e+00]\n",
            " ...\n",
            " [9.67528176e-05 0.00000000e+00 3.35024414e-03 ... 0.00000000e+00\n",
            "  9.79075074e-01 1.00000000e+00]\n",
            " [9.35461649e-05 0.00000000e+00 3.22412490e-03 ... 0.00000000e+00\n",
            "  9.79895592e-01 1.00000000e+00]\n",
            " [9.94690955e-01 1.00000000e+00 5.00829984e-03 ... 0.00000000e+00\n",
            "  3.06116749e-06 0.00000000e+00]]\n",
            "1/4 [======>.......................] - ETA: 0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 65ms/step\n",
            "yhat:  [[9.68891982e-05 3.35637340e-03 5.89827728e-03 1.16152111e-02\n",
            "  9.79033113e-01]\n",
            " [7.65519799e-05 2.53086118e-03 4.42599319e-03 8.94282386e-03\n",
            "  9.84023690e-01]\n",
            " [2.06928793e-02 3.10279101e-01 5.80528855e-01 5.42895421e-02\n",
            "  3.42096351e-02]\n",
            " [9.67522792e-05 3.35021294e-03 5.88381151e-03 1.15938159e-02\n",
            "  9.79075253e-01]\n",
            " [8.92506287e-05 2.37224414e-03 3.69793014e-03 7.96466321e-03\n",
            "  9.85875905e-01]\n",
            " [2.17432663e-01 6.18489385e-01 1.48567051e-01 1.05923982e-02\n",
            "  4.91853850e-03]\n",
            " [3.18944663e-01 5.74841321e-01 9.04377252e-02 9.25908703e-03\n",
            "  6.51714532e-03]\n",
            " [9.57839075e-05 2.43442948e-03 3.72009748e-03 8.07298627e-03\n",
            "  9.85676587e-01]\n",
            " [9.94691253e-01 5.00813406e-03 2.59535998e-04 3.81116333e-05\n",
            "  3.06111588e-06]\n",
            " [9.67617816e-05 3.35063459e-03 5.88479964e-03 1.15952808e-02\n",
            "  9.79072452e-01]\n",
            " [9.67900560e-05 3.35182971e-03 5.88754099e-03 1.15993768e-02\n",
            "  9.79064405e-01]\n",
            " [9.67689484e-05 3.35095939e-03 5.88556053e-03 1.15964040e-02\n",
            "  9.79070246e-01]\n",
            " [9.68187233e-05 3.35324113e-03 5.89095289e-03 1.16043622e-02\n",
            "  9.79054630e-01]\n",
            " [9.24400229e-05 3.18368524e-03 5.53528359e-03 1.10394545e-02\n",
            "  9.80149031e-01]\n",
            " [9.67840388e-05 3.35158105e-03 5.88697568e-03 1.15985228e-02\n",
            "  9.79066074e-01]\n",
            " [2.31735525e-03 7.67669082e-02 2.23789826e-01 8.75949115e-02\n",
            "  6.09530985e-01]\n",
            " [2.52573818e-01 6.12985611e-01 1.21775776e-01 8.67995899e-03\n",
            "  3.98478471e-03]\n",
            " [2.12316349e-01 6.18738174e-01 1.52960554e-01 1.09093385e-02\n",
            "  5.07562421e-03]\n",
            " [9.67631277e-05 3.35070025e-03 5.88495657e-03 1.15955081e-02\n",
            "  9.79072034e-01]\n",
            " [9.67529995e-05 3.35025229e-03 5.88390557e-03 1.15939509e-02\n",
            "  9.79075074e-01]\n",
            " [9.94596839e-01 5.09622646e-03 2.65096314e-04 3.88125882e-05\n",
            "  3.13228747e-06]\n",
            " [9.67829692e-05 3.35141597e-03 5.88650582e-03 1.15978792e-02\n",
            "  9.79067326e-01]\n",
            " [9.67645828e-05 3.35073634e-03 5.88502269e-03 1.15956152e-02\n",
            "  9.79071796e-01]\n",
            " [9.05849229e-05 3.10889864e-03 5.37530798e-03 1.07870596e-02\n",
            "  9.80638027e-01]\n",
            " [9.67519227e-05 3.35020362e-03 5.88379288e-03 1.15937833e-02\n",
            "  9.79075372e-01]\n",
            " [9.94692206e-01 5.00717433e-03 2.59490480e-04 3.81063219e-05\n",
            "  3.06050879e-06]\n",
            " [2.52901256e-01 6.12907708e-01 1.21551000e-01 8.66337959e-03\n",
            "  3.97656485e-03]\n",
            " [8.58505227e-05 2.92738969e-03 5.01164189e-03 1.01856682e-02\n",
            "  9.81789351e-01]\n",
            " [2.06806343e-02 3.10198903e-01 5.80594480e-01 5.43031991e-02\n",
            "  3.42227146e-02]\n",
            " [9.84449625e-01 1.47224711e-02 7.24155223e-04 9.30392052e-05\n",
            "  1.06201878e-05]\n",
            " [9.67784872e-05 3.35134077e-03 5.88641595e-03 1.15976976e-02\n",
            "  9.79067802e-01]\n",
            " [9.67573287e-05 3.35043063e-03 5.88431489e-03 1.15945637e-02\n",
            "  9.79074001e-01]\n",
            " [9.57855373e-05 2.43446836e-03 3.72016593e-03 8.07310827e-03\n",
            "  9.85676348e-01]\n",
            " [7.76997331e-05 2.63598864e-03 4.55968222e-03 9.28209629e-03\n",
            "  9.83444631e-01]\n",
            " [9.67565938e-05 3.35040712e-03 5.88426506e-03 1.15944874e-02\n",
            "  9.79074121e-01]\n",
            " [9.35356002e-05 3.22383293e-03 5.61475148e-03 1.11707132e-02\n",
            "  9.79897082e-01]\n",
            " [3.84033145e-03 1.19414009e-01 3.61824930e-01 1.03273652e-01\n",
            "  4.11647081e-01]\n",
            " [2.22802624e-01 6.18071258e-01 1.44095302e-01 1.02709150e-02\n",
            "  4.75995569e-03]\n",
            " [9.67734450e-05 3.35113774e-03 5.88596612e-03 1.15970103e-02\n",
            "  9.79069114e-01]\n",
            " [9.06446148e-05 3.11153498e-03 5.38140582e-03 1.07962368e-02\n",
            "  9.80620027e-01]\n",
            " [9.67540836e-05 3.35029769e-03 5.88401081e-03 1.15941139e-02\n",
            "  9.79074836e-01]\n",
            " [2.52534568e-01 6.12994969e-01 1.21802717e-01 8.68194923e-03\n",
            "  3.98578076e-03]\n",
            " [9.67567830e-05 3.35041201e-03 5.88427344e-03 1.15945041e-02\n",
            "  9.79074121e-01]\n",
            " [9.67583837e-05 3.35048162e-03 5.88443549e-03 1.15947463e-02\n",
            "  9.79073524e-01]\n",
            " [9.94691253e-01 5.00811543e-03 2.59528577e-04 3.81103928e-05\n",
            "  3.06101106e-06]\n",
            " [9.69045213e-05 3.35651333e-03 5.89822559e-03 1.16153639e-02\n",
            "  9.79032874e-01]\n",
            " [9.94106889e-01 5.55269653e-03 2.94260826e-04 4.24735481e-05\n",
            "  3.50933806e-06]\n",
            " [9.94600058e-01 5.09321596e-03 2.64913455e-04 3.87888103e-05\n",
            "  3.12981024e-06]\n",
            " [2.06839386e-02 3.10219914e-01 5.80577433e-01 5.42996004e-02\n",
            "  3.42191905e-02]\n",
            " [9.94690299e-01 5.00907283e-03 2.59586988e-04 3.81179561e-05\n",
            "  3.06179618e-06]\n",
            " [9.94690537e-01 5.00882091e-03 2.59574212e-04 3.81164391e-05\n",
            "  3.06162769e-06]\n",
            " [9.67630403e-05 3.35070025e-03 5.88495657e-03 1.15955081e-02\n",
            "  9.79072034e-01]\n",
            " [9.67515662e-05 3.35019128e-03 5.88376820e-03 1.15937460e-02\n",
            "  9.79075491e-01]\n",
            " [9.57835582e-05 2.43442412e-03 3.72009119e-03 8.07297602e-03\n",
            "  9.85676706e-01]\n",
            " [8.75443802e-05 2.35973322e-03 3.70277278e-03 7.95166101e-03\n",
            "  9.85898256e-01]\n",
            " [9.94692087e-01 5.00719529e-03 2.59477063e-04 3.81038844e-05\n",
            "  3.06031870e-06]\n",
            " [9.83970940e-01 1.51290065e-02 7.87586323e-04 1.01042038e-04\n",
            "  1.15398416e-05]\n",
            " [9.67754095e-05 3.35124554e-03 5.88622876e-03 1.15973940e-02\n",
            "  9.79068398e-01]\n",
            " [9.94690537e-01 5.00875851e-03 2.59568507e-04 3.81156024e-05\n",
            "  3.06155175e-06]\n",
            " [9.67646629e-05 3.35075497e-03 5.88506972e-03 1.15956804e-02\n",
            "  9.79071677e-01]\n",
            " [8.68875286e-05 2.96678348e-03 5.08867577e-03 1.03155132e-02\n",
            "  9.81542230e-01]\n",
            " [9.68351160e-05 3.35386954e-03 5.89234894e-03 1.16064753e-02\n",
            "  9.79050398e-01]\n",
            " [9.20154416e-05 3.16448230e-03 5.49040455e-03 1.09724244e-02\n",
            "  9.80280578e-01]\n",
            " [9.67537126e-05 3.35028162e-03 5.88397449e-03 1.15940543e-02\n",
            "  9.79074836e-01]\n",
            " [9.68450622e-05 3.35444417e-03 5.89379156e-03 1.16085475e-02\n",
            "  9.79046345e-01]\n",
            " [9.94690180e-01 5.00912452e-03 2.59590423e-04 3.81184582e-05\n",
            "  3.06184825e-06]\n",
            " [9.36085926e-05 3.22701945e-03 5.62209170e-03 1.11817336e-02\n",
            "  9.79875505e-01]\n",
            " [2.53175348e-01 6.12851620e-01 1.21355705e-01 8.64832290e-03\n",
            "  3.96898761e-03]\n",
            " [9.94690299e-01 5.00906305e-03 2.59587745e-04 3.81181744e-05\n",
            "  3.06181096e-06]\n",
            " [9.94690299e-01 5.00898669e-03 2.59581808e-04 3.81173377e-05\n",
            "  3.06172910e-06]\n",
            " [7.67064266e-05 2.59192172e-03 4.54552518e-03 9.16794688e-03\n",
            "  9.83617902e-01]\n",
            " [9.67919405e-05 3.35202902e-03 5.88809652e-03 1.16001386e-02\n",
            "  9.79062915e-01]\n",
            " [2.53325760e-01 6.12816274e-01 1.21252120e-01 8.64066835e-03\n",
            "  3.96519853e-03]\n",
            " [9.94691014e-01 5.00832452e-03 2.59548076e-04 3.81132959e-05\n",
            "  3.06128459e-06]\n",
            " [9.94690299e-01 5.00896294e-03 2.59582041e-04 3.81174104e-05\n",
            "  3.06173206e-06]\n",
            " [9.94690180e-01 5.00911986e-03 2.59592169e-04 3.81187892e-05\n",
            "  3.06187189e-06]\n",
            " [9.67857122e-05 3.35158804e-03 5.88694541e-03 1.15985069e-02\n",
            "  9.79066193e-01]\n",
            " [9.94691253e-01 5.00802230e-03 2.59525375e-04 3.81100690e-05\n",
            "  3.06097013e-06]\n",
            " [8.37344705e-05 2.34634406e-03 3.75366001e-03 7.98246637e-03\n",
            "  9.85833824e-01]\n",
            " [9.67649976e-05 3.35079525e-03 5.88518241e-03 1.15958424e-02\n",
            "  9.79071319e-01]\n",
            " [9.94693041e-01 5.00630029e-03 2.59428081e-04 3.80978199e-05\n",
            "  3.05967092e-06]\n",
            " [9.67729211e-05 3.35110817e-03 5.88589208e-03 1.15969079e-02\n",
            "  9.79069352e-01]\n",
            " [2.52590925e-01 6.12982035e-01 1.21763639e-01 8.67904350e-03\n",
            "  3.98433208e-03]\n",
            " [9.67541710e-05 3.35029606e-03 5.88400010e-03 1.15940981e-02\n",
            "  9.79074836e-01]\n",
            " [9.67918968e-05 3.35197640e-03 5.88793913e-03 1.15999291e-02\n",
            "  9.79063392e-01]\n",
            " [9.67784435e-05 3.35138571e-03 5.88656170e-03 1.15978848e-02\n",
            "  9.79067326e-01]\n",
            " [1.73721388e-01 6.15888774e-01 1.90131694e-01 1.37289744e-02\n",
            "  6.52930886e-03]\n",
            " [9.94690061e-01 5.00923162e-03 2.59596593e-04 3.81192185e-05\n",
            "  3.06192692e-06]\n",
            " [9.67547967e-05 3.35033028e-03 5.88408858e-03 1.15942229e-02\n",
            "  9.79074597e-01]\n",
            " [9.94690418e-01 5.00887539e-03 2.59575143e-04 3.81165082e-05\n",
            "  3.06164475e-06]\n",
            " [9.51789698e-05 3.28904949e-03 5.75417560e-03 1.13897100e-02\n",
            "  9.79471922e-01]\n",
            " [9.94690537e-01 5.00883302e-03 2.59574212e-04 3.81164391e-05\n",
            "  3.06163042e-06]\n",
            " [9.94690180e-01 5.00917481e-03 2.59594904e-04 3.81191494e-05\n",
            "  3.06191259e-06]\n",
            " [2.46193051e-01 6.14449620e-01 1.26225129e-01 8.99512786e-03\n",
            "  4.13699355e-03]\n",
            " [9.67533706e-05 3.35026183e-03 5.88392001e-03 1.15939798e-02\n",
            "  9.79075074e-01]\n",
            " [9.67559608e-05 3.35038197e-03 5.88421244e-03 1.15944063e-02\n",
            "  9.79074240e-01]\n",
            " [9.77316797e-01 2.14494504e-02 1.08508661e-03 1.31806271e-04\n",
            "  1.68699953e-05]\n",
            " [9.67862288e-05 3.35162180e-03 5.88703854e-03 1.15986364e-02\n",
            "  9.79065835e-01]\n",
            " [9.77316797e-01 2.14494411e-02 1.08508661e-03 1.31806140e-04\n",
            "  1.68699953e-05]\n",
            " [9.05853594e-05 3.10892123e-03 5.37536200e-03 1.07871415e-02\n",
            "  9.80638027e-01]\n",
            " [9.94690776e-01 5.00858342e-03 2.59559631e-04 3.81145910e-05\n",
            "  3.06143261e-06]\n",
            " [9.67840388e-05 3.35158431e-03 5.88698359e-03 1.15985395e-02\n",
            "  9.79066074e-01]\n",
            " [9.07734939e-05 3.11709568e-03 5.39418310e-03 1.08155105e-02\n",
            "  9.80582416e-01]\n",
            " [9.94692087e-01 5.00720739e-03 2.59476103e-04 3.81037389e-05\n",
            "  3.06031575e-06]\n",
            " [8.48628770e-05 2.89038336e-03 4.94126603e-03 1.00646159e-02\n",
            "  9.82018769e-01]\n",
            " [9.94690418e-01 5.00894943e-03 2.59579130e-04 3.81169411e-05\n",
            "  3.06169136e-06]\n",
            " [8.69774813e-05 2.97074276e-03 5.09779295e-03 1.03293434e-02\n",
            "  9.81515050e-01]\n",
            " [9.67524538e-05 3.35022993e-03 5.88385342e-03 1.15938755e-02\n",
            "  9.79075134e-01]\n",
            " [9.21304672e-05 3.16968630e-03 5.50255785e-03 1.09905945e-02\n",
            "  9.80245054e-01]\n",
            " [1.30371759e-02 2.00921550e-01 2.97842264e-01 8.18905681e-02\n",
            "  4.06308502e-01]\n",
            " [9.93164003e-01 6.43038657e-03 3.51678464e-04 4.97480905e-05\n",
            "  4.28636395e-06]\n",
            " [7.94739535e-05 2.39140773e-03 3.97623423e-03 8.27368349e-03\n",
            "  9.85279202e-01]\n",
            " [1.68738469e-01 6.15441263e-01 1.94856733e-01 1.41761284e-02\n",
            "  6.78743748e-03]\n",
            " [5.12739876e-04 1.52426092e-02 3.27264331e-02 2.98735350e-02\n",
            "  9.21644688e-01]\n",
            " [8.15447347e-05 2.76786997e-03 4.72019706e-03 9.66904685e-03\n",
            "  9.82761323e-01]\n",
            " [9.94310319e-01 5.36338240e-03 2.82113324e-04 4.09503409e-05\n",
            "  3.35126811e-06]\n",
            " [9.68213499e-05 3.35320877e-03 5.89076430e-03 1.16041554e-02\n",
            "  9.79054987e-01]\n",
            " [9.67653614e-05 3.35080340e-03 5.88519406e-03 1.15958648e-02\n",
            "  9.79071319e-01]\n",
            " [9.57837328e-05 2.43442715e-03 3.72009748e-03 8.07298627e-03\n",
            "  9.85676587e-01]\n",
            " [9.67528176e-05 3.35024414e-03 5.88388927e-03 1.15939295e-02\n",
            "  9.79075074e-01]\n",
            " [9.35461649e-05 3.22412490e-03 5.61520876e-03 1.11715700e-02\n",
            "  9.79895592e-01]\n",
            " [9.94690955e-01 5.00829984e-03 2.59540160e-04 3.81118734e-05\n",
            "  3.06116749e-06]]\n",
            "<class 'numpy.ndarray'>\n",
            "yhat:  [[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]]\n",
            "<class 'numpy.ndarray'>\n",
            "Stacked Test Accuracy: 0.951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.metrics.mean_squared_error(testlab,yhat))\n",
        "print(sklearn.metrics.mean_absolute_error(testlab,yhat))\n",
        "print(sklearn.metrics.f1_score(testlab,yhat,average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tenvzDmfdRHY",
        "outputId": "d1b10122-1dc6-407c-f6a4-ddf34b2eed9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11475409836065574\n",
            "0.06557377049180328\n",
            "0.9440784964797895\n"
          ]
        }
      ]
    }
  ]
}