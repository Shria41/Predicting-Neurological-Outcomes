{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xEMTwHcrACV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('features_dwt.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize EEG data (110 columns)\n",
        "eeg_data = df.iloc[:, 7:14].values  # Assuming columns 7 to 116 are EEG data\n",
        "scaler_eeg = MinMaxScaler()\n",
        "eeg_data_normalized = scaler_eeg.fit_transform(eeg_data)\n",
        "static_features = df.iloc[:, :7].values\n",
        "# Normalize static features\n",
        "scaler_static = MinMaxScaler()\n",
        "static_features_normalized = scaler_static.fit_transform(static_features)\n",
        "# Combine normalized static features, normalized EEG data, and CPC values\n",
        "normalized_data = np.concatenate((static_features_normalized, eeg_data_normalized), axis=1)\n",
        "\n",
        "# Convert CPC values to one-hot encoded labels\n",
        "one_hot_labels = to_categorical(cpc_values - 1, num_classes=5)  # Assuming CPC values start from 1\n",
        "# Train-test split\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    normalized_data, one_hot_labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Fqfp1R3Xqd7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "U1CZLVOinTem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Gaussian Process\",\n",
        "         \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n",
        "         \"Naive Bayes\"]\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(5),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
        "    DecisionTreeClassifier(max_depth=3),\n",
        "    RandomForestClassifier(max_depth=3, n_estimators=10),\n",
        "    AdaBoostClassifier(),\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "svH6HWWVo49s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testlab=[]\n",
        "for i in test_labels.tolist():\n",
        "  testlab.append([i.index(1.0)+1])\n",
        "trainlab=[]\n",
        "for i in train_labels.tolist():\n",
        "  trainlab.append([i.index(1.0)+1])"
      ],
      "metadata": {
        "id": "Sxnnd3D-n-uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "# Define model\n",
        "model1 = Sequential()\n",
        "model1.add(LSTM(units=128, input_shape=(14, 1), return_sequences=True))  # Input shape should match the number of EEG columns (117)\n",
        "model1.add(Dropout(0.3))  # Add dropout layer with a dropout rate of 30%\n",
        "model1.add(LSTM(units=64, return_sequences=True))\n",
        "model1.add(LSTM(units=32))\n",
        "model1.add(Dense(units=16, activation='relu'))\n",
        "model1.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "t799agGCoLzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dwt-norm\n",
        "model1.fit(x=train_data, y=train_labels, epochs=30, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKmHVJlTrBuu",
        "outputId": "206be457-56b4-4513-d8a9-265ab214b26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "13/13 [==============================] - 9s 159ms/step - loss: 1.4645 - accuracy: 0.5556 - val_loss: 1.0808 - val_accuracy: 0.6082\n",
            "Epoch 2/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0745 - accuracy: 0.5788 - val_loss: 1.0218 - val_accuracy: 0.6082\n",
            "Epoch 3/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0542 - accuracy: 0.5788 - val_loss: 1.0211 - val_accuracy: 0.6082\n",
            "Epoch 4/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0490 - accuracy: 0.5788 - val_loss: 1.0139 - val_accuracy: 0.6082\n",
            "Epoch 5/30\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 1.0481 - accuracy: 0.5788 - val_loss: 1.0101 - val_accuracy: 0.6082\n",
            "Epoch 6/30\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.0458 - accuracy: 0.5788 - val_loss: 1.0058 - val_accuracy: 0.6082\n",
            "Epoch 7/30\n",
            "13/13 [==============================] - 1s 56ms/step - loss: 1.0433 - accuracy: 0.5788 - val_loss: 1.0046 - val_accuracy: 0.6082\n",
            "Epoch 8/30\n",
            "13/13 [==============================] - 1s 73ms/step - loss: 1.0425 - accuracy: 0.5788 - val_loss: 1.0005 - val_accuracy: 0.6082\n",
            "Epoch 9/30\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 1.0425 - accuracy: 0.5788 - val_loss: 0.9994 - val_accuracy: 0.6082\n",
            "Epoch 10/30\n",
            "13/13 [==============================] - 1s 66ms/step - loss: 1.0397 - accuracy: 0.5788 - val_loss: 0.9980 - val_accuracy: 0.6082\n",
            "Epoch 11/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0436 - accuracy: 0.5788 - val_loss: 1.0035 - val_accuracy: 0.6082\n",
            "Epoch 12/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0411 - accuracy: 0.5788 - val_loss: 0.9910 - val_accuracy: 0.6082\n",
            "Epoch 13/30\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 1.0394 - accuracy: 0.5788 - val_loss: 0.9891 - val_accuracy: 0.6082\n",
            "Epoch 14/30\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 1.0372 - accuracy: 0.5788 - val_loss: 0.9909 - val_accuracy: 0.6082\n",
            "Epoch 15/30\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 1.0378 - accuracy: 0.5788 - val_loss: 0.9927 - val_accuracy: 0.6082\n",
            "Epoch 16/30\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 1.0365 - accuracy: 0.5788 - val_loss: 0.9889 - val_accuracy: 0.6082\n",
            "Epoch 17/30\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.0343 - accuracy: 0.5788 - val_loss: 0.9847 - val_accuracy: 0.6082\n",
            "Epoch 18/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0354 - accuracy: 0.5788 - val_loss: 0.9821 - val_accuracy: 0.6082\n",
            "Epoch 19/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0333 - accuracy: 0.5788 - val_loss: 0.9852 - val_accuracy: 0.6082\n",
            "Epoch 20/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0355 - accuracy: 0.5788 - val_loss: 0.9730 - val_accuracy: 0.6082\n",
            "Epoch 21/30\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 1.0308 - accuracy: 0.5788 - val_loss: 0.9737 - val_accuracy: 0.6082\n",
            "Epoch 22/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0324 - accuracy: 0.5788 - val_loss: 0.9683 - val_accuracy: 0.6082\n",
            "Epoch 23/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0299 - accuracy: 0.5788 - val_loss: 0.9710 - val_accuracy: 0.6082\n",
            "Epoch 24/30\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.0311 - accuracy: 0.5788 - val_loss: 0.9659 - val_accuracy: 0.6082\n",
            "Epoch 25/30\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.0293 - accuracy: 0.5788 - val_loss: 0.9643 - val_accuracy: 0.6082\n",
            "Epoch 26/30\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 1.0249 - accuracy: 0.5788 - val_loss: 0.9560 - val_accuracy: 0.6082\n",
            "Epoch 27/30\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.0209 - accuracy: 0.5788 - val_loss: 0.9612 - val_accuracy: 0.6082\n",
            "Epoch 28/30\n",
            "13/13 [==============================] - 1s 72ms/step - loss: 1.0097 - accuracy: 0.5788 - val_loss: 0.9423 - val_accuracy: 0.6082\n",
            "Epoch 29/30\n",
            "13/13 [==============================] - 1s 75ms/step - loss: 0.9934 - accuracy: 0.5788 - val_loss: 0.9168 - val_accuracy: 0.6082\n",
            "Epoch 30/30\n",
            "13/13 [==============================] - 1s 77ms/step - loss: 0.9644 - accuracy: 0.5788 - val_loss: 0.8730 - val_accuracy: 0.6082\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d24d13afb50>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model1.evaluate(x=test_data, y=test_labels)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "809ENoTWtDv_",
        "outputId": "2b6fb7b6-5385-43f9-f4ae-f815050e4152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 14ms/step - loss: 1.0209 - accuracy: 0.5738\n",
            "Test Loss: 1.0209163427352905, Test Accuracy: 0.5737704634666443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from math import exp\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = pd.read_csv('features_all.csv')\n",
        "data = data.dropna()\n",
        "X = data.iloc[:, 1:-1].values\n",
        "y = data['CPC'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "gem_matrix_train = np.zeros((X_train_scaled.shape[0], X_train_scaled.shape[0]))\n",
        "for i in range(X_train_scaled.shape[0]):\n",
        "    for j in range(i, X_train_scaled.shape[0]):\n",
        "        dtw_distance = np.sqrt(np.sum(np.square(X_train_scaled[i] - X_train_scaled[j])))\n",
        "        gem_matrix_train[i, j] = exp(-dtw_distance ** 2)\n",
        "        gem_matrix_train[j, i] = gem_matrix_train[i, j]\n",
        "\n",
        "gem_matrix_test = np.zeros((X_test_scaled.shape[0], X_train_scaled.shape[0]))\n",
        "for i in range(X_test_scaled.shape[0]):\n",
        "    for j in range(X_train_scaled.shape[0]):\n",
        "        dtw_distance = np.sqrt(np.sum(np.square(X_test_scaled[i] - X_train_scaled[j])))\n",
        "        gem_matrix_test[i, j] = exp(-dtw_distance ** 2)\n",
        "\n",
        "clf = SVC(kernel='precomputed')\n",
        "clf.fit(gem_matrix_train, y_train)\n",
        "y_pred = clf.predict(gem_matrix_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "3se2med18Yv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f0c6a8-d42e-4a81-b7ab-8b71013d79c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dwt\n",
        "# stacked generalization with linear meta model on blobs dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from numpy import dstack\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "members=[model1,clf]\n",
        "# load models from file\n",
        "'''def load_all_models(n_models):\n",
        "\tall_models = list()\n",
        "\tfor i in range(n_models):\n",
        "\t\t# define filename for this ensemble\n",
        "\t\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
        "\t\t# load model from file\n",
        "\t\tmodel = load_model(filename)\n",
        "\t\t# add to list of members\n",
        "\t\tall_models.append(model)\n",
        "\t\tprint('>loaded %s' % filename)\n",
        "\treturn all_models'''\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX):\n",
        "  stackX = None\n",
        "  for model in members:\n",
        "    # make prediction\n",
        "    if(model!=clf):\n",
        "      yhat = model.predict(inputX)\n",
        "      print('yhat: ',yhat)\n",
        "      print(type(yhat))\n",
        "    else:\n",
        "      gem_matrix_test = np.zeros((inputX.shape[0], train_data.shape[0]))\n",
        "      for i in range(inputX.shape[0]):\n",
        "          for j in range(train_data.shape[0]):\n",
        "              dtw_distance = np.sqrt(np.sum(np.square(inputX[i] - train_data[j])))\n",
        "              gem_matrix_test[i, j] = exp(-dtw_distance ** 2)\n",
        "      yhat = model.predict(gem_matrix_test)\n",
        "      yhat1=[]\n",
        "      for i in yhat.tolist():\n",
        "        l=[0.0,0.0,0.0,0.0,0.0]\n",
        "        l[i-1]=1.0\n",
        "        yhat1.append(l)\n",
        "      yhat=np.array(yhat1)\n",
        "      print('yhat: ',yhat)\n",
        "      print(type(yhat))\n",
        "    # stack predictions into [rows, members, probabilities]\n",
        "    if stackX is None:\n",
        "      stackX = yhat\n",
        "    else:\n",
        "      stackX = dstack((stackX, yhat))\n",
        "\t# flatten predictions to [rows, members x probabilities]\n",
        "  stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "  return stackX\n",
        "\n",
        "#[1 0 0 0 0]\n",
        "#[0 1 0 0 0]\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model(members, inputX, inputy):\n",
        "  # create dataset using ensemble\n",
        "  stackedX = stacked_dataset(members, inputX)\n",
        "  print('stacked: ',stackedX)\n",
        "  # fit standalone model\n",
        "  #model = LogisticRegression()\n",
        "  #model=clf\n",
        "  model=classifiers[5]\n",
        "  #odel=knn1\n",
        "  #model=rfmodel ----->give 100 percent accuracy\n",
        "  model.fit(stackedX, inputy)\n",
        "  return model\n",
        "\n",
        "# make a prediction with the stacked model\n",
        "def stacked_prediction(members, model, inputX):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# make a prediction\n",
        "\tyhat = model.predict(stackedX)\n",
        "\treturn yhat\n",
        "\n",
        "# fit stacked model using the ensemble\n",
        "model_final = fit_stacked_model(members, test_data, testlab)\n",
        "# evaluate model on test set\n",
        "yhat = stacked_prediction(members, model_final, test_data)\n",
        "acc = accuracy_score(testlab, yhat)\n",
        "print('Stacked Test Accuracy: %.3f' % acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX6hY5uVtbkm",
        "outputId": "dee49a58-5b92-497c-a5f4-756667b5563d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 12ms/step\n",
            "yhat:  [[0.24669482 0.06660986 0.02884811 0.02231034 0.6355369 ]\n",
            " [0.24643439 0.0665826  0.02877615 0.02228015 0.63592666]\n",
            " [0.28646728 0.07420335 0.03961238 0.02599846 0.5737185 ]\n",
            " [0.24665868 0.06661652 0.02883951 0.02230841 0.63557684]\n",
            " [0.24657778 0.06660117 0.02880467 0.02228697 0.63572943]\n",
            " [0.30891827 0.07793819 0.04683165 0.02803066 0.53828126]\n",
            " [0.30889007 0.07795057 0.04682192 0.02802959 0.5383078 ]\n",
            " [0.2467255  0.06663075 0.02886672 0.02232483 0.6354523 ]\n",
            " [0.32940876 0.08120795 0.05438764 0.02992846 0.50506717]\n",
            " [0.24661928 0.06658425 0.02881814 0.02229161 0.6356868 ]\n",
            " [0.24666278 0.06657838 0.02882165 0.02228788 0.63564944]\n",
            " [0.24662319 0.0665893  0.0288168  0.02229029 0.6356803 ]\n",
            " [0.24686645 0.06672543 0.02895207 0.02239106 0.63506496]\n",
            " [0.24681208 0.06668508 0.02891739 0.02236355 0.63522196]\n",
            " [0.2467527  0.06664326 0.02888142 0.02233546 0.6353872 ]\n",
            " [0.26464844 0.0702781  0.03347512 0.02404634 0.60755193]\n",
            " [0.30891067 0.07798919 0.04687119 0.02806335 0.5381655 ]\n",
            " [0.30893227 0.0779929  0.04689688 0.02807776 0.53810006]\n",
            " [0.24663658 0.06659591 0.02882197 0.02229366 0.63565177]\n",
            " [0.24663231 0.06657088 0.0288107  0.02228163 0.6357045 ]\n",
            " [0.3293608  0.08122122 0.05437391 0.02992896 0.5051151 ]\n",
            " [0.24673757 0.06665615 0.02888741 0.02234436 0.6353745 ]\n",
            " [0.2466609  0.0665694  0.02881361 0.02227995 0.6356762 ]\n",
            " [0.24666889 0.06661829 0.02883381 0.02230187 0.63557714]\n",
            " [0.24660288 0.06658497 0.02880669 0.02228361 0.63572186]\n",
            " [0.32942784 0.08118366 0.05438119 0.02991928 0.5050879 ]\n",
            " [0.30893585 0.07799118 0.04687754 0.02806387 0.5381315 ]\n",
            " [0.24664637 0.06657528 0.02881652 0.0222853  0.6356765 ]\n",
            " [0.2861841  0.07416406 0.03954186 0.02598479 0.5741252 ]\n",
            " [0.32896653 0.08107686 0.05416309 0.02985061 0.50594294]\n",
            " [0.24671206 0.06659088 0.02883873 0.02229744 0.6355609 ]\n",
            " [0.24658518 0.06655763 0.02879823 0.02227588 0.635783  ]\n",
            " [0.24666953 0.06659966 0.02883448 0.02230063 0.6355958 ]\n",
            " [0.24663998 0.066632   0.02884426 0.02231696 0.63556683]\n",
            " [0.24653248 0.06657739 0.0287825  0.02227109 0.63583654]\n",
            " [0.24680616 0.06668232 0.02891497 0.02236195 0.6352345 ]\n",
            " [0.26466665 0.07019614 0.03343414 0.0240015  0.6077016 ]\n",
            " [0.30883136 0.07794629 0.04680526 0.02802712 0.53838986]\n",
            " [0.24664348 0.06659906 0.02882703 0.02229758 0.6356328 ]\n",
            " [0.24669492 0.06665244 0.02886543 0.02233019 0.63545704]\n",
            " [0.2466698  0.06659678 0.02883535 0.02230112 0.635597  ]\n",
            " [0.30889195 0.0779562  0.04682494 0.02803185 0.53829503]\n",
            " [0.24668981 0.0666329  0.02885447 0.02231907 0.63550365]\n",
            " [0.24663778 0.06656507 0.02880797 0.02227778 0.63571143]\n",
            " [0.3293403  0.08118177 0.05431487 0.02989257 0.5052705 ]\n",
            " [0.24670021 0.06660573 0.02883643 0.02229847 0.6355591 ]\n",
            " [0.32943818 0.0812257  0.05443883 0.02995667 0.50494057]\n",
            " [0.3294589  0.08118751 0.05439757 0.02992464 0.50503135]\n",
            " [0.28625658 0.07416808 0.0395483  0.02597893 0.5740481 ]\n",
            " [0.32942012 0.08118778 0.05438349 0.02992232 0.5050863 ]\n",
            " [0.32942325 0.08115979 0.0543402  0.02989277 0.50518405]\n",
            " [0.24665143 0.06660324 0.02882989 0.02229943 0.63561594]\n",
            " [0.24669151 0.06661177 0.0288485  0.02231136 0.63553685]\n",
            " [0.24672624 0.06663796 0.02886883 0.02232732 0.63543963]\n",
            " [0.24666178 0.06661581 0.02883821 0.0223067  0.63557744]\n",
            " [0.32945824 0.08121465 0.0544394  0.02995281 0.50493485]\n",
            " [0.32941416 0.08117407 0.05435725 0.02990596 0.50514865]\n",
            " [0.2466595  0.06657908 0.02882392 0.0222905  0.635647  ]\n",
            " [0.32943222 0.08120057 0.05440961 0.0299375  0.5050202 ]\n",
            " [0.24657856 0.06658325 0.02879997 0.02228086 0.63575745]\n",
            " [0.24676433 0.06666953 0.02889972 0.02235295 0.63531333]\n",
            " [0.24667667 0.06662135 0.02884747 0.02231352 0.6355411 ]\n",
            " [0.24677825 0.06666435 0.02889782 0.02234868 0.6353108 ]\n",
            " [0.24665374 0.06661741 0.02883737 0.02230727 0.6355842 ]\n",
            " [0.24661028 0.06658816 0.02881182 0.02228751 0.6357022 ]\n",
            " [0.3294256  0.08115933 0.05434707 0.02989656 0.5051715 ]\n",
            " [0.24662453 0.06661015 0.02883201 0.02230596 0.63562727]\n",
            " [0.308901   0.07795073 0.04683155 0.02803443 0.53828233]\n",
            " [0.3294357  0.08122812 0.05444158 0.02995898 0.50493556]\n",
            " [0.3293129  0.08124585 0.05439494 0.02995181 0.50509447]\n",
            " [0.24656351 0.06661866 0.02881197 0.02229738 0.63570845]\n",
            " [0.24666905 0.06660164 0.0288382  0.02230428 0.63558686]\n",
            " [0.3089679  0.07801767 0.04693656 0.02810136 0.5379765 ]\n",
            " [0.32943165 0.08116282 0.05435615 0.02990147 0.50514793]\n",
            " [0.3289479  0.08111929 0.05420775 0.0298848  0.5058402 ]\n",
            " [0.32948133 0.08118592 0.05441953 0.02993435 0.50497895]\n",
            " [0.24680184 0.06664522 0.02887698 0.02232436 0.63535166]\n",
            " [0.32940432 0.08121385 0.05440861 0.02994257 0.50503063]\n",
            " [0.24680853 0.06668873 0.02892084 0.02236758 0.6352144 ]\n",
            " [0.24667071 0.0666451  0.02886258 0.02233045 0.63549125]\n",
            " [0.32946363 0.08117769 0.05439623 0.0299222  0.5050403 ]\n",
            " [0.2468042  0.06669395 0.02892138 0.0223692  0.63521117]\n",
            " [0.30373237 0.07721738 0.04515631 0.02762674 0.54626715]\n",
            " [0.24664012 0.06660724 0.0288278  0.02229955 0.6356252 ]\n",
            " [0.2466677  0.06660129 0.02883421 0.02230081 0.6355961 ]\n",
            " [0.24672963 0.06661922 0.02886287 0.02231961 0.63546866]\n",
            " [0.30893618 0.07791573 0.04681306 0.02801317 0.5383219 ]\n",
            " [0.32943982 0.08121057 0.0544173  0.02994188 0.5049905 ]\n",
            " [0.24677739 0.06666835 0.02889949 0.02235071 0.63530415]\n",
            " [0.32947505 0.0811632  0.05439252 0.0299168  0.50505245]\n",
            " [0.24666461 0.06659114 0.02882544 0.02229228 0.63562644]\n",
            " [0.32928228 0.08114765 0.05432875 0.02990751 0.5053337 ]\n",
            " [0.32882768 0.08114228 0.05416369 0.02987919 0.5059871 ]\n",
            " [0.3088436  0.07795933 0.04679826 0.02802159 0.53837717]\n",
            " [0.24669905 0.06661088 0.02884803 0.02230975 0.63553226]\n",
            " [0.24658525 0.06656781 0.02879599 0.02227479 0.63577616]\n",
            " [0.3294194  0.0811905  0.05438333 0.02992256 0.5050842 ]\n",
            " [0.24680372 0.06663724 0.02887414 0.02232075 0.6353642 ]\n",
            " [0.32941988 0.08119202 0.05438561 0.02992401 0.50507843]\n",
            " [0.246777   0.06666613 0.0288982  0.02234938 0.6353092 ]\n",
            " [0.32940894 0.08119603 0.05438761 0.02992736 0.50508004]\n",
            " [0.24661715 0.06656915 0.02880786 0.0222811  0.6357248 ]\n",
            " [0.24665205 0.0665839  0.02882397 0.0222921  0.6356479 ]\n",
            " [0.32943895 0.08116359 0.05437264 0.02991047 0.5051144 ]\n",
            " [0.24670382 0.06661493 0.02885288 0.02231386 0.63551456]\n",
            " [0.32940334 0.08116925 0.05435296 0.02990466 0.5051698 ]\n",
            " [0.24653032 0.0665618  0.02879142 0.02227809 0.6358383 ]\n",
            " [0.24666187 0.06658733 0.02882738 0.02229409 0.63562924]\n",
            " [0.24675366 0.0666505  0.02888582 0.02234001 0.63537   ]\n",
            " [0.2861905  0.07415884 0.03953654 0.02597929 0.5741347 ]\n",
            " [0.32918224 0.08111672 0.05429753 0.02990185 0.50550175]\n",
            " [0.2466622  0.06662238 0.02884272 0.02231138 0.6355612 ]\n",
            " [0.30829206 0.07786459 0.04659177 0.0279592  0.5392924 ]\n",
            " [0.26484042 0.07034729 0.03353078 0.0240708  0.60721064]\n",
            " [0.24662755 0.06659881 0.02881618 0.02228998 0.63566756]\n",
            " [0.32941243 0.08117752 0.05436559 0.02991167 0.50513273]\n",
            " [0.2468234  0.06664813 0.02889974 0.02234226 0.63528645]\n",
            " [0.24661945 0.06659003 0.02881422 0.02228854 0.63568777]\n",
            " [0.24667135 0.06659252 0.02883251 0.02229787 0.63560563]\n",
            " [0.2467301  0.06663141 0.02886841 0.02232575 0.6354444 ]\n",
            " [0.24664705 0.06664642 0.0288503  0.02232281 0.6355334 ]\n",
            " [0.32941929 0.08117054 0.0543553  0.02990362 0.5051513 ]]\n",
            "<class 'numpy.ndarray'>\n",
            "yhat:  [[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]]\n",
            "<class 'numpy.ndarray'>\n",
            "stacked:  [[0.24669482 0.         0.06660986 ... 0.         0.63553691 1.        ]\n",
            " [0.24643439 0.         0.0665826  ... 0.         0.63592666 1.        ]\n",
            " [0.28646728 0.         0.07420335 ... 0.         0.57371849 0.        ]\n",
            " ...\n",
            " [0.2467301  0.         0.06663141 ... 0.         0.6354444  1.        ]\n",
            " [0.24664705 0.         0.06664642 ... 0.         0.63553339 1.        ]\n",
            " [0.32941929 1.         0.08117054 ... 0.         0.50515127 0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 18ms/step\n",
            "yhat:  [[0.24669482 0.06660986 0.02884811 0.02231034 0.6355369 ]\n",
            " [0.24643439 0.0665826  0.02877615 0.02228015 0.63592666]\n",
            " [0.28646728 0.07420335 0.03961238 0.02599846 0.5737185 ]\n",
            " [0.24665868 0.06661652 0.02883951 0.02230841 0.63557684]\n",
            " [0.24657778 0.06660117 0.02880467 0.02228697 0.63572943]\n",
            " [0.30891827 0.07793819 0.04683165 0.02803066 0.53828126]\n",
            " [0.30889007 0.07795057 0.04682192 0.02802959 0.5383078 ]\n",
            " [0.2467255  0.06663075 0.02886672 0.02232483 0.6354523 ]\n",
            " [0.32940876 0.08120795 0.05438764 0.02992846 0.50506717]\n",
            " [0.24661928 0.06658425 0.02881814 0.02229161 0.6356868 ]\n",
            " [0.24666278 0.06657838 0.02882165 0.02228788 0.63564944]\n",
            " [0.24662319 0.0665893  0.0288168  0.02229029 0.6356803 ]\n",
            " [0.24686645 0.06672543 0.02895207 0.02239106 0.63506496]\n",
            " [0.24681208 0.06668508 0.02891739 0.02236355 0.63522196]\n",
            " [0.2467527  0.06664326 0.02888142 0.02233546 0.6353872 ]\n",
            " [0.26464844 0.0702781  0.03347512 0.02404634 0.60755193]\n",
            " [0.30891067 0.07798919 0.04687119 0.02806335 0.5381655 ]\n",
            " [0.30893227 0.0779929  0.04689688 0.02807776 0.53810006]\n",
            " [0.24663658 0.06659591 0.02882197 0.02229366 0.63565177]\n",
            " [0.24663231 0.06657088 0.0288107  0.02228163 0.6357045 ]\n",
            " [0.3293608  0.08122122 0.05437391 0.02992896 0.5051151 ]\n",
            " [0.24673757 0.06665615 0.02888741 0.02234436 0.6353745 ]\n",
            " [0.2466609  0.0665694  0.02881361 0.02227995 0.6356762 ]\n",
            " [0.24666889 0.06661829 0.02883381 0.02230187 0.63557714]\n",
            " [0.24660288 0.06658497 0.02880669 0.02228361 0.63572186]\n",
            " [0.32942784 0.08118366 0.05438119 0.02991928 0.5050879 ]\n",
            " [0.30893585 0.07799118 0.04687754 0.02806387 0.5381315 ]\n",
            " [0.24664637 0.06657528 0.02881652 0.0222853  0.6356765 ]\n",
            " [0.2861841  0.07416406 0.03954186 0.02598479 0.5741252 ]\n",
            " [0.32896653 0.08107686 0.05416309 0.02985061 0.50594294]\n",
            " [0.24671206 0.06659088 0.02883873 0.02229744 0.6355609 ]\n",
            " [0.24658518 0.06655763 0.02879823 0.02227588 0.635783  ]\n",
            " [0.24666953 0.06659966 0.02883448 0.02230063 0.6355958 ]\n",
            " [0.24663998 0.066632   0.02884426 0.02231696 0.63556683]\n",
            " [0.24653248 0.06657739 0.0287825  0.02227109 0.63583654]\n",
            " [0.24680616 0.06668232 0.02891497 0.02236195 0.6352345 ]\n",
            " [0.26466665 0.07019614 0.03343414 0.0240015  0.6077016 ]\n",
            " [0.30883136 0.07794629 0.04680526 0.02802712 0.53838986]\n",
            " [0.24664348 0.06659906 0.02882703 0.02229758 0.6356328 ]\n",
            " [0.24669492 0.06665244 0.02886543 0.02233019 0.63545704]\n",
            " [0.2466698  0.06659678 0.02883535 0.02230112 0.635597  ]\n",
            " [0.30889195 0.0779562  0.04682494 0.02803185 0.53829503]\n",
            " [0.24668981 0.0666329  0.02885447 0.02231907 0.63550365]\n",
            " [0.24663778 0.06656507 0.02880797 0.02227778 0.63571143]\n",
            " [0.3293403  0.08118177 0.05431487 0.02989257 0.5052705 ]\n",
            " [0.24670021 0.06660573 0.02883643 0.02229847 0.6355591 ]\n",
            " [0.32943818 0.0812257  0.05443883 0.02995667 0.50494057]\n",
            " [0.3294589  0.08118751 0.05439757 0.02992464 0.50503135]\n",
            " [0.28625658 0.07416808 0.0395483  0.02597893 0.5740481 ]\n",
            " [0.32942012 0.08118778 0.05438349 0.02992232 0.5050863 ]\n",
            " [0.32942325 0.08115979 0.0543402  0.02989277 0.50518405]\n",
            " [0.24665143 0.06660324 0.02882989 0.02229943 0.63561594]\n",
            " [0.24669151 0.06661177 0.0288485  0.02231136 0.63553685]\n",
            " [0.24672624 0.06663796 0.02886883 0.02232732 0.63543963]\n",
            " [0.24666178 0.06661581 0.02883821 0.0223067  0.63557744]\n",
            " [0.32945824 0.08121465 0.0544394  0.02995281 0.50493485]\n",
            " [0.32941416 0.08117407 0.05435725 0.02990596 0.50514865]\n",
            " [0.2466595  0.06657908 0.02882392 0.0222905  0.635647  ]\n",
            " [0.32943222 0.08120057 0.05440961 0.0299375  0.5050202 ]\n",
            " [0.24657856 0.06658325 0.02879997 0.02228086 0.63575745]\n",
            " [0.24676433 0.06666953 0.02889972 0.02235295 0.63531333]\n",
            " [0.24667667 0.06662135 0.02884747 0.02231352 0.6355411 ]\n",
            " [0.24677825 0.06666435 0.02889782 0.02234868 0.6353108 ]\n",
            " [0.24665374 0.06661741 0.02883737 0.02230727 0.6355842 ]\n",
            " [0.24661028 0.06658816 0.02881182 0.02228751 0.6357022 ]\n",
            " [0.3294256  0.08115933 0.05434707 0.02989656 0.5051715 ]\n",
            " [0.24662453 0.06661015 0.02883201 0.02230596 0.63562727]\n",
            " [0.308901   0.07795073 0.04683155 0.02803443 0.53828233]\n",
            " [0.3294357  0.08122812 0.05444158 0.02995898 0.50493556]\n",
            " [0.3293129  0.08124585 0.05439494 0.02995181 0.50509447]\n",
            " [0.24656351 0.06661866 0.02881197 0.02229738 0.63570845]\n",
            " [0.24666905 0.06660164 0.0288382  0.02230428 0.63558686]\n",
            " [0.3089679  0.07801767 0.04693656 0.02810136 0.5379765 ]\n",
            " [0.32943165 0.08116282 0.05435615 0.02990147 0.50514793]\n",
            " [0.3289479  0.08111929 0.05420775 0.0298848  0.5058402 ]\n",
            " [0.32948133 0.08118592 0.05441953 0.02993435 0.50497895]\n",
            " [0.24680184 0.06664522 0.02887698 0.02232436 0.63535166]\n",
            " [0.32940432 0.08121385 0.05440861 0.02994257 0.50503063]\n",
            " [0.24680853 0.06668873 0.02892084 0.02236758 0.6352144 ]\n",
            " [0.24667071 0.0666451  0.02886258 0.02233045 0.63549125]\n",
            " [0.32946363 0.08117769 0.05439623 0.0299222  0.5050403 ]\n",
            " [0.2468042  0.06669395 0.02892138 0.0223692  0.63521117]\n",
            " [0.30373237 0.07721738 0.04515631 0.02762674 0.54626715]\n",
            " [0.24664012 0.06660724 0.0288278  0.02229955 0.6356252 ]\n",
            " [0.2466677  0.06660129 0.02883421 0.02230081 0.6355961 ]\n",
            " [0.24672963 0.06661922 0.02886287 0.02231961 0.63546866]\n",
            " [0.30893618 0.07791573 0.04681306 0.02801317 0.5383219 ]\n",
            " [0.32943982 0.08121057 0.0544173  0.02994188 0.5049905 ]\n",
            " [0.24677739 0.06666835 0.02889949 0.02235071 0.63530415]\n",
            " [0.32947505 0.0811632  0.05439252 0.0299168  0.50505245]\n",
            " [0.24666461 0.06659114 0.02882544 0.02229228 0.63562644]\n",
            " [0.32928228 0.08114765 0.05432875 0.02990751 0.5053337 ]\n",
            " [0.32882768 0.08114228 0.05416369 0.02987919 0.5059871 ]\n",
            " [0.3088436  0.07795933 0.04679826 0.02802159 0.53837717]\n",
            " [0.24669905 0.06661088 0.02884803 0.02230975 0.63553226]\n",
            " [0.24658525 0.06656781 0.02879599 0.02227479 0.63577616]\n",
            " [0.3294194  0.0811905  0.05438333 0.02992256 0.5050842 ]\n",
            " [0.24680372 0.06663724 0.02887414 0.02232075 0.6353642 ]\n",
            " [0.32941988 0.08119202 0.05438561 0.02992401 0.50507843]\n",
            " [0.246777   0.06666613 0.0288982  0.02234938 0.6353092 ]\n",
            " [0.32940894 0.08119603 0.05438761 0.02992736 0.50508004]\n",
            " [0.24661715 0.06656915 0.02880786 0.0222811  0.6357248 ]\n",
            " [0.24665205 0.0665839  0.02882397 0.0222921  0.6356479 ]\n",
            " [0.32943895 0.08116359 0.05437264 0.02991047 0.5051144 ]\n",
            " [0.24670382 0.06661493 0.02885288 0.02231386 0.63551456]\n",
            " [0.32940334 0.08116925 0.05435296 0.02990466 0.5051698 ]\n",
            " [0.24653032 0.0665618  0.02879142 0.02227809 0.6358383 ]\n",
            " [0.24666187 0.06658733 0.02882738 0.02229409 0.63562924]\n",
            " [0.24675366 0.0666505  0.02888582 0.02234001 0.63537   ]\n",
            " [0.2861905  0.07415884 0.03953654 0.02597929 0.5741347 ]\n",
            " [0.32918224 0.08111672 0.05429753 0.02990185 0.50550175]\n",
            " [0.2466622  0.06662238 0.02884272 0.02231138 0.6355612 ]\n",
            " [0.30829206 0.07786459 0.04659177 0.0279592  0.5392924 ]\n",
            " [0.26484042 0.07034729 0.03353078 0.0240708  0.60721064]\n",
            " [0.24662755 0.06659881 0.02881618 0.02228998 0.63566756]\n",
            " [0.32941243 0.08117752 0.05436559 0.02991167 0.50513273]\n",
            " [0.2468234  0.06664813 0.02889974 0.02234226 0.63528645]\n",
            " [0.24661945 0.06659003 0.02881422 0.02228854 0.63568777]\n",
            " [0.24667135 0.06659252 0.02883251 0.02229787 0.63560563]\n",
            " [0.2467301  0.06663141 0.02886841 0.02232575 0.6354444 ]\n",
            " [0.24664705 0.06664642 0.0288503  0.02232281 0.6355334 ]\n",
            " [0.32941929 0.08117054 0.0543553  0.02990362 0.5051513 ]]\n",
            "<class 'numpy.ndarray'>\n",
            "yhat:  [[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]]\n",
            "<class 'numpy.ndarray'>\n",
            "Stacked Test Accuracy: 0.943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.metrics.mean_squared_error(testlab,yhat))\n",
        "print(sklearn.metrics.mean_absolute_error(testlab,yhat))\n",
        "print(sklearn.metrics.f1_score(testlab,yhat,average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiLCmYWPsQ_1",
        "outputId": "640c8979-5faf-4430-db54-d8ef5a226536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13114754098360656\n",
            "0.08196721311475409\n",
            "0.9200198708395431\n"
          ]
        }
      ]
    }
  ]
}